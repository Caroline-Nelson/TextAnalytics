{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics Group Assignment 2\n",
    "\n",
    "Nicole Erich, Vishwa Bhuta, Caroline Nelson, Erik Honore, Lindsay Tober"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pre-work\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Setup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Import Statements - Basic\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Statements - nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import collocations\n",
    "from nltk import sentiment\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.chunk.regexp import *\n",
    "\n",
    "# # Just in case\n",
    "# nltk.download()\n",
    "\n",
    "# Import Statements - sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import Statements - PMI calculations\n",
    "import re\n",
    "import math\n",
    "from math import log\n",
    "from decimal import Decimal\n",
    "from collections import defaultdict\n",
    "from django.utils.encoding import smart_str, smart_unicode\n",
    "import string\n",
    "\n",
    "# Import Statements - Modeling & Other\n",
    "from patsy import dmatrices\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "import functools32\n",
    "\n",
    "# Import Statements - Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('Yelp Data Restaurant Reviews Ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task A.\n",
    "\n",
    "### Ignore the text (reviews) and run a classification model with the numeric data (you can use standard methods like logistic regression, k-nearest neighbors or anything else). What is the best accuracy of your model with numeric data?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the data better, we first wanted to know if the different cuisine types are a dummy variable of \"cuisine,\" or if a restaurant could be categorized into multiple.  Finding that the second option was often true, we allowed for each cuisine type to act as its own feature.  Similarly, we also confirmed that each review could only select one price range.  We confirmed that there were no null values to contend with, and created a new \"Target\" column that assigned a value of \"high\" or \"low\" to each review based on the star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Data Validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stars', 'votes_cool', 'votes_funny', 'votes_useful', 'Cheap',\n",
       "       'Moderate', 'Expensive', 'VeryExpensive', 'American', 'Chinese',\n",
       "       'French', 'Japanese', 'Indian', 'Italian', 'Greek', 'Mediterranean',\n",
       "       'Mexican', 'Thai', 'Vietnamese', 'Others', 'Review'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    7395\n",
       "5    6158\n",
       "3    3353\n",
       "2    1940\n",
       "1    1153\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuisine Validation\n",
      "1    17307\n",
      "2     2343\n",
      "3      331\n",
      "4       18\n",
      "Name: sumcuisine, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check number of cuisines listed per review\n",
    "cuisine_list = list(data.iloc[:,8:20])\n",
    "data['sumcuisine'] = data[cuisine_list].sum(axis=1)\n",
    "checkcuisinesperreview = data['sumcuisine'].value_counts()\n",
    "print 'Cuisine Validation'\n",
    "print checkcuisinesperreview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price Validation\n",
      "1    19999\n",
      "Name: sumprice, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check number of prices listed per review\n",
    "price_list = list(data.iloc[:,4:8])\n",
    "data['sumprice'] = data[price_list].sum(axis=1)\n",
    "checkpriceperreview = data['sumprice'].value_counts()\n",
    "print 'Price Validation'\n",
    "print checkpriceperreview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Validation\n",
      "Column 0 0\n",
      "Column 1 0\n",
      "Column 2 0\n",
      "Column 3 0\n",
      "Column 4 0\n",
      "Column 5 0\n",
      "Column 6 0\n",
      "Column 7 0\n",
      "Column 8 0\n",
      "Column 9 0\n",
      "Column 10 0\n",
      "Column 11 0\n",
      "Column 12 0\n",
      "Column 13 0\n",
      "Column 14 0\n",
      "Column 15 0\n",
      "Column 16 0\n",
      "Column 17 0\n",
      "Column 18 0\n",
      "Column 19 0\n",
      "Column 20 0\n",
      "Column 21 0\n",
      "Column 22 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print 'Null Validation'\n",
    "for i in range(0,len(data.columns.values)):\n",
    "    print \"Column\", i,\n",
    "    print sum(pd.isnull(data.ix[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    13553\n",
      "0.0     6446\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>6158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target   0.0   1.0\n",
       "stars             \n",
       "1       1153     0\n",
       "2       1940     0\n",
       "3       3353     0\n",
       "4          0  7395\n",
       "5          0  6158"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set 'target' to 1 for 4* and 5* reviews and 0 for 3* and below\n",
    "# Validate results\n",
    "data['target'] = 0.0\n",
    "data['target'][data['stars'] > 3] = 1.0\n",
    "print data['target'].value_counts()\n",
    "check_target = pd.crosstab(data['stars'],data['target'])\n",
    "check_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['votes_cool', 'votes_funny', 'votes_useful', 'Cheap', 'Moderate',\n",
       "       'Expensive', 'VeryExpensive', 'American', 'Chinese', 'French',\n",
       "       'Japanese', 'Indian', 'Italian', 'Greek', 'Mediterranean',\n",
       "       'Mexican', 'Thai', 'Vietnamese', 'Others'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dmatrices, select only numerical values for X and target column for Y\n",
    "X_A = data.ix[:,1:20]\n",
    "y_A = data['target']\n",
    "X_A.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split test / train data\n",
    "X_train_LR, X_test_LR, y_train_LR, y_test_LR = train_test_split(X_A, y_A, \n",
    "                                                                test_size=0.5, \n",
    "                                                                random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create and fit a logistic regression model\n",
    "regr = linear_model.LogisticRegression()\n",
    "regr.fit(X_train_LR, y_train_LR)\n",
    "\n",
    "# Use model to make predictions\n",
    "predictions_LR = regr.predict(X_test_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions\n",
    "predictions_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 160 3065]\n",
      " [ 101 6674]]\n"
     ]
    }
   ],
   "source": [
    "# Print Confusion matrix\n",
    "confusion_matrix_LR = metrics.confusion_matrix(y_test_LR, predictions_LR, labels=unique(y_A))\n",
    "print confusion_matrix_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6834\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "print metrics.accuracy_score(y_test_LR, predictions_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3557 more high than low, so we predict \"High.\"\n",
      "Baseline accuracy would be 0.6779\n"
     ]
    }
   ],
   "source": [
    "# Baseline accuracy\n",
    "baseline_LR = round(1.0*sum([y_train_LR == 1])/len(y_train_LR),4)\n",
    "\n",
    "print \"There are\", sum([y_train_LR == 1]) - sum([y_train_LR == 0]), \\\n",
    "    \"more high than low, so we predict \\\"High.\\\"\"\n",
    "\n",
    "print \"Baseline accuracy would be\", baseline_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic regression model beats the baseline model by 0.0055\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression model comparison to baseline\n",
    "print \"The logistic regression model beats the baseline model by\", \\\n",
    "    metrics.accuracy_score(y_test_LR, predictions_LR) - baseline_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | Task A - Results |\n",
    "We trained a logistic regression model on a subset of data to predict either high or low.  On a test set, we achieved an accuracy of approximately 68.34% using this method.  For a comparison, a baseline accuracy would be to only predict the majority class ('High').  This baseline model on a given test set achieves approximately 67.75% accuracy.  The logistic regression model using only numeric data (ignoring the review text) performs only slightly better than the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task B.\n",
    "\n",
    "### Perform a supervised classification on a subset of the corpus using the reviews only. You can write your code in Python or R. What accuracy do you get from this text mining exercise?\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Unigram Method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample for faster processing\n",
    "random.seed(20)\n",
    "data_sample = data.sample(n=5000,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dmatrices, select only reviews for X and target column for Y\n",
    "X_B = data_sample['Review']\n",
    "y_B = data_sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(X_B, y_B, \n",
    "                                                            test_size=0.25, \n",
    "                                                            random_state=20)\n",
    "\n",
    "# Create arrays for vectorizer input\n",
    "X_train_B = np.array(X_train_B)\n",
    "y_train_B = np.array(y_train_B)\n",
    "X_test_B = np.array(X_test_B)\n",
    "y_test_B = np.array(y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build vectorizor to get TF-IDF scores\n",
    "vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform X's for training and test sets\n",
    "X_train_UM = vectorizer.fit_transform(X_train_B)\n",
    "X_test_UM = vectorizer.transform(X_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 16803)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_UM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit Naive Bayes Multinomial\n",
    "NBclassifier_UM = MultinomialNB().fit(X_train_UM, y_train_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use NB to get predictions\n",
    "predictions_nb_UM = NBclassifier_UM.predict(X_test_UM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 399]\n",
      " [  0 849]]\n"
     ]
    }
   ],
   "source": [
    "# Print Confusion matrix\n",
    "confusion_matrix_UM = metrics.confusion_matrix(y_test_B, predictions_nb_UM, labels=unique(y_B))\n",
    "print confusion_matrix_UM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6808\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for NB all words\n",
    "accuracy_UM = metrics.accuracy_score(y_test_B, predictions_nb_UM)\n",
    "print accuracy_UM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Lemmatization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building tokenizer and lemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for item in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(item))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize training and test set X's\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "X_train_TFIDF = tfidf.fit_transform(X_train_B)\n",
    "X_test_TFIDF = tfidf.transform(X_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit naive bayes multinomial model with tokenized training set\n",
    "NBclassifier_TFIDF = MultinomialNB().fit(X_train_TFIDF, y_train_B)\n",
    "predictions_nb_TFIDF = NBclassifier_TFIDF.predict(X_test_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1 400]\n",
      " [  0 849]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix for NB with tokenization\n",
    "confusion_matrix_TFIDF = metrics.confusion_matrix(y_test_B, predictions_nb_TFIDF)\n",
    "print confusion_matrix_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy score for NB with tokenization\n",
    "accuracy_TFIDF = metrics.accuracy_score(y_test_B, predictions_nb_TFIDF)\n",
    "print accuracy_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Remove Stop-Words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create vectorizer that removes stop words\n",
    "vectorizer_stop = TfidfVectorizer(min_df=0.05, \n",
    "                                  ngram_range=(1, 1), \n",
    "                                  stop_words='english', \n",
    "                                  strip_accents='unicode', \n",
    "                                  norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform X's for training and test sets (same but with stop words)\n",
    "X_train_RSW = vectorizer_stop.fit_transform(X_train_B)\n",
    "X_test_RSW = vectorizer_stop.transform(X_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit Naive Bayes Multinomial\n",
    "NBclassifier_RSW = MultinomialNB().fit(X_train_RSW, y_train_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use NB to get predictions\n",
    "predictions_nb_RSW = NBclassifier_RSW.predict(X_test_RSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 83 318]\n",
      " [ 17 832]]\n"
     ]
    }
   ],
   "source": [
    "# Print Confusion matrix\n",
    "confusion_matrix_RSW = metrics.confusion_matrix(y_test_B, predictions_nb_RSW, \n",
    "                                                labels=unique(y_B))\n",
    "print confusion_matrix_RSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.732\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for NB no stop words\n",
    "accuracy_RSW = metrics.accuracy_score(y_test_B, predictions_nb_RSW)\n",
    "print accuracy_RSW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Part-of-Speech Bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'(<(NN[^\\\\{\\\\}<>]*)>)(<(VB[^\\\\{\\\\}<>]*)>)|(<(JJ)>)(<(NN)>)|(<(NNP)>)(<(VB)>)|(<(NNP)>)(<(VBP)>)|(<(NN)>)(<(VB)>)|(<(VB)>)(<(NNP)>)|(<(VB)>)(<(NN)>)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS Tag Pattern\n",
    "tag_pattern = \"<NN.*><VB.*>|<JJ><NN>|<NNP><VB>|<NNP><VBP>|<NN><VB>|<VB><NNP>|<VB><NN>\"\n",
    "regexp_pattern = tag_pattern2re_pattern(tag_pattern)\n",
    "regexp_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize Function\n",
    "def tokenize1(text):\n",
    "    tokens_unigrams = nltk.word_tokenize(text)\n",
    "    bigram_tokenizer = nltk.tokenize.regexp.RegexpTokenizer(regexp_pattern)\n",
    "    tokens_bigrams = bigram_tokenizer.tokenize(text)\n",
    "    tokens= tokens_unigrams+tokens_bigrams\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get POS Bigrams\n",
    "tfidf_bigram = TfidfVectorizer(tokenizer=tokenize1)\n",
    "X_train_POSBG = tfidf_bigram.fit_transform(X_train_B)\n",
    "X_test_POSBG = tfidf_bigram.transform(X_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict using POS Bigrams\n",
    "NBclassifier_POSBG = MultinomialNB().fit(X_train_POSBG, y_train_B)\n",
    "predictions_nb_POSBG = NBclassifier_POSBG.predict(X_test_POSBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1 400]\n",
      " [  0 849]]\n"
     ]
    }
   ],
   "source": [
    "# Print Confusion matrix\n",
    "confusion_matrix_POSBG = metrics.confusion_matrix(y_test_B, predictions_nb_POSBG)\n",
    "print confusion_matrix_POSBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for NB POS Bigrams\n",
    "accuracy_POSBG = metrics.accuracy_score(y_test_B, predictions_nb_POSBG)\n",
    "print accuracy_POSBG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | Task B - Results |\n",
    "For part B, we applied different approaches of text mining to build multiple models.  Overall, we found that Naive Bayes performed better on average than logistic regression, so each text mining example uses a NB to test the accuracy.  The methods tested were: all unigrams, lemmatization, removing stop words (and limiting min-df to 0.05), a stop words/limiting min-df and lemmatization model, and part-of-speech bigrams.  Of all of these methods, the removing stop words model consistently performed the best.  Adding lemmatization to this method complicated the model without increasing accuracy, so it is not the final choice.\n",
    "\n",
    "We found that the biggest impact to accuracy was changing the min-df constraint in the stop-words model.  It was able to remove words that were only in 5% of all reviews, thereby reducing complexity as well as increasing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task C. \n",
    "\n",
    "### Combine the numeric data and the text classification model (in task B) to create a “hybrid” model. It is your task to figure out how to do this. Now run this hybrid classification model and compare the results with those in A and B. Does the numeric data add to the predictive power relative to text?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['votes_cool', 'votes_funny', 'votes_useful', 'Cheap', 'Moderate',\n",
       "       'Expensive', 'VeryExpensive', 'American', 'Chinese', 'French',\n",
       "       'Japanese', 'Indian', 'Italian', 'Greek', 'Mediterranean',\n",
       "       'Mexican', 'Thai', 'Vietnamese', 'Others', 'Review'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set X and y for model\n",
    "X_C = data_sample.ix[:,1:21]\n",
    "y_C = data_sample['target']\n",
    "X_C.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_train_C, X_test_C, y_train_C, y_test_C = train_test_split(X_C, y_C, \n",
    "                                                            test_size=0.25, \n",
    "                                                            random_state=20)\n",
    "\n",
    "# Create arrays for vectorizer input\n",
    "X_train_C = np.array(X_train_C)\n",
    "y_train_C = np.array(y_train_C)\n",
    "X_test_C = np.array(X_test_C)\n",
    "y_test_C = np.array(y_test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform X's for training and test sets with stop words\n",
    "X_train_TXT = vectorizer_stop.fit_transform(X_train_C[:,-1])\n",
    "X_test_TXT = vectorizer_stop.transform(X_test_C[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 19)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate X_train dimensions....\n",
    "X_train_C[:,0:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 189)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is congruent with X_train3 dimensions (rows)\n",
    "X_train_TXT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn all into sparse (coo) matrices\n",
    "\n",
    "A = coo_matrix(X_train_C[:,0:-1])\n",
    "B = coo_matrix(X_train_TXT)\n",
    "\n",
    "C = coo_matrix(X_test_C[:,0:-1])\n",
    "D = coo_matrix(X_test_TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine numeric and vectorized text data into one matrix\n",
    "X_train_HYBRID = sparse.hstack([A.astype(float),B]).toarray()\n",
    "X_test_HYBRID = sparse.hstack([C.astype(float),D]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 208)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the resulting X_train_all dimensions for desired result\n",
    "X_train_HYBRID.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit and predict using a Multinomial NB approach\n",
    "NBclassifier_HYBRID = MultinomialNB().fit(X_train_HYBRID, y_train_C)\n",
    "predictions_HYBRID_nb = NBclassifier_HYBRID.predict(X_test_HYBRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107 294]\n",
      " [ 39 810]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix for NB on HYBRID model\n",
    "confusion_matrix_HYBRID_nb = metrics.confusion_matrix(y_test_C, predictions_HYBRID_nb)\n",
    "print confusion_matrix_HYBRID_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7336\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy score for NB on HYBRID model\n",
    "accuracy_HYBRID_nb = metrics.accuracy_score(y_test_C, predictions_HYBRID_nb)\n",
    "print accuracy_HYBRID_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [KNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit and predict using a KNN model\n",
    "KNNclassifier_HYBRID = KNeighborsClassifier().fit(X_train_HYBRID, y_train_C) \n",
    "predictions_HYBRID_knn = KNNclassifier_HYBRID.predict(X_test_HYBRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[108 293]\n",
      " [116 733]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix for KNN on HYBRID data\n",
    "confusion_matrix_HYBRID_knn = metrics.confusion_matrix(y_test_C, predictions_HYBRID_knn)\n",
    "print confusion_matrix_HYBRID_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6728\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy score for KNN on HYBRID data\n",
    "accuracy_HYBRID_knn = metrics.accuracy_score(y_test_C, predictions_HYBRID_knn)\n",
    "print accuracy_HYBRID_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | Task C - Results |\n",
    "To create a hybrid model, we first decided to simply attach the review features as additional columns to the dataset of numerical values for each row.  After selecting the remove-stop-words model in Part B as the most accurate, this is how we transformed both the training and test set X's.\n",
    "\n",
    "On this first hybrid model, we ran both a Naive Bayes model as well as a K-nearest-neighbors, to ensure that NB would still be a good selection for a hybrid dataset.  As expected, Naive Bayes performed much better. For a particular random split, Naive Bayes predicted with a 75.6% accuracy, while KNN only predicted 66.2% correctly.\n",
    "\n",
    "To improve this model, we hypothesized that the approximately 200 columns generated by the text vectorizer may be \"drowning\" the data from the first 19 numerical columns.   We decided to use predictions generated from text data to form an additional column to the original 19.  To do this, we used the Naive Bayes fit from the removing-stop-words vectorizer to create the new prediction column.  With a new dataset of just 20 columns, we made our final predictions with both Naive Bayes and a Logistic Regression. In this case, Logistic Regression performed better.  Using this model, we achieved 74.24% accuracy on the test set.  Unfortunately, it did not beat the previous method as expected.\n",
    "\n",
    "It's possible that the second model under-weighted the effect of the text reviews in comparing it equally with the other feature columns.  To improve the model, we would need to find the optimal balance between the numeric and text features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task D. \n",
    "\n",
    "### Use unsupervised sentiment analysis on the reviews (with SentiStrength or any other tool) and use the sentiment scores to predict high/low rating. Compare and contrast the results of tasks B and D. What can you conclude from your analysis?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['stars', 'votes_cool', 'votes_funny', 'votes_useful', 'Cheap',\n",
       "       'Moderate', 'Expensive', 'VeryExpensive', 'American', 'Chinese',\n",
       "       'French', 'Japanese', 'Indian', 'Italian', 'Greek', 'Mediterranean',\n",
       "       'Mexican', 'Thai', 'Vietnamese', 'Others', 'Review', 'Difference'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data with SentiStrength analysis ('Difference' column)\n",
    "data_SS = pd.read_csv('Yelp Data Restaurant Reviews Ratings (1).csv')\n",
    "\n",
    "# View columns to confirm 'Difference' imported\n",
    "data_SS.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    13553\n",
      "0.0     6446\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>6158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target   0.0   1.0\n",
       "stars             \n",
       "1       1153     0\n",
       "2       1940     0\n",
       "3       3353     0\n",
       "4          0  7395\n",
       "5          0  6158"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set 'target' to 1 for 4* and 5* reviews and 0 for 3* and below\n",
    "# Validate results\n",
    "data_SS['target'] = 0.0\n",
    "data_SS['target'][data_SS['stars'] > 3] = 1.0\n",
    "print data_SS['target'].value_counts()\n",
    "check_target_SS = pd.crosstab(data_SS['stars'],data_SS['target'])\n",
    "check_target_SS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Logistic Regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create test and train sets\n",
    "train_SS, test_SS = train_test_split(data_SS, test_size=.33, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create matrices for test and train sets difference calculation variables\n",
    "y_train_SS,X_train_SS = dmatrices('target~0+Difference', train_SS)\n",
    "y_test_SS,X_test_SS = dmatrices('target~0+Difference', test_SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Logistic Regression model and predict on test set\n",
    "LRclassifier_SS = LogisticRegression().fit(X_train_SS,y_train_SS)\n",
    "predictions_SS_lr = LRclassifier_SS.predict(X_test_SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 2168]\n",
      " [   0 4431]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrix for Logistic Regression on SentiStrength data\n",
    "confusion_matrix_SS_lr = metrics.confusion_matrix(y_test_SS, predictions_SS_lr)\n",
    "print confusion_matrix_SS_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671465373541\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy score for Logistic Regression on SentiStrength data\n",
    "accuracy_SS_lr = metrics.accuracy_score(y_test_SS, predictions_SS_lr)\n",
    "print accuracy_SS_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | Task D - Results |\n",
    "We decided to use the difference between positive and negative sentiment scores to determine the overall positive or negative score of each review (column named 'Difference').  We then ran a logistic regression with only the 'Difference' column against high and low ratings, and saw an accuracy of 67.14% on the test subset.  In Part B, using Naive Bayes while removing stopwords, and removing words that did not appear in 5% of the documents, the accuracy score was between 68% and 74%.  The accuracy score could have dropped due to the lack of data behind the classification, and skewed by the number of high and low ratings in the original data set.  Using fewer words to calculate the overall sentiment score could have benefitted our logistic regression model.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task E. \n",
    "\n",
    "### Implement the PMI approach to sentiment analysis (in either Python or R), and run the classification model with the sentiment scores. How do your results compare with those in Task D?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get subset for analysis\n",
    "random.seed(20)\n",
    "data_SS_subset = data_SS.sample(n=2000,replace=False)\n",
    "data_SS_subset = data_SS_subset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bag of words for subset of data\n",
    "list_of_words=[]\n",
    "for i in range(len(data_SS_subset)):\n",
    "    new_text = data_SS_subset['Review'][i].decode('utf-8')\n",
    "    text = word_tokenize(new_text)\n",
    "    list_of_words+=text;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Prepare Sentiment Reference Terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in text files with reference terms for positive and negative sentiments\n",
    "refpos = pd.read_csv('positive-words.txt',header=None)\n",
    "refneg = pd.read_csv('negative-words.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn reference words into lists\n",
    "refposList= refpos[0].tolist()\n",
    "refnegList= refneg[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the lists of positive and negative reference words that occur in our corpus\n",
    "posMatches = list(set(list_of_words).intersection(set(refposList)))\n",
    "negMatches = list(set(list_of_words).intersection(set(refnegList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Prepare Part-of-Speech Bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get lowercase version of reviews for consistency\n",
    "data_SS_subset['reviews'] = data_SS_subset['Review'].apply(lambda t: filter(lambda x: x in string.printable, t))\n",
    "data_SS_subset['reviews'] = data_SS_subset['reviews'].apply(lambda t: t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to tag each word with POS\n",
    "def tag_review(review):\n",
    "    new_text= review\n",
    "    text = word_tokenize(new_text)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    #tagged = [word + \"/\" + tag for (word, tag) in tagged]\n",
    "    #string = ' '.join(tagged)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply POS tagger function to Review column\n",
    "data_SS_subset['TaggedReview'] = data_SS_subset['reviews'].apply(tag_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get list of applicable bigrams\n",
    "def get_bigrams(review):\n",
    "    list_of_bigrams=[]\n",
    "    for (w1,t1), (w2,t2) in nltk.bigrams(review):\n",
    "        if (t1.startswith('J') and t2.startswith('N')):\n",
    "            list_of_bigrams.append((w1, w2))\n",
    "        elif (t1.startswith('R') and t2.startswith('J')):\n",
    "            list_of_bigrams.append((w1, w2))\n",
    "        elif (t1.startswith('N') and t2.startswith('J')):\n",
    "            list_of_bigrams.append((w1, w2))\n",
    "    return list_of_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply Bigrams function to Tagged Reviews column\n",
    "list_of_bigrams = data_SS_subset['TaggedReview'].apply(get_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Prepare Training Set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates train and test data on SentiStrength subset\n",
    "train_SSsubset, test_SSsubset = train_test_split(data_SS_subset,\n",
    "                                                 test_size = .33,\n",
    "                                                 random_state = 16)\n",
    "\n",
    "train_SSsubset = train_SSsubset.reset_index()\n",
    "test_SSsubset = test_SSsubset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull values from 'Review' column\n",
    "train_SSsubset_reviewslist = train_SSsubset['reviews'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes words that are in less than 5% and more than 98% of docs\n",
    "train_vect = CountVectorizer(min_df=0.05, max_df=0.98, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating count matrix for training set\n",
    "train_CountVect = train_vect.fit_transform(train_SSsubset_reviewslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts count matrix into count dataframe\n",
    "train_count_df = pd.SparseDataFrame([pd.SparseSeries(train_CountVect[i].toarray().ravel()) \n",
    "                                     for i in np.arange(train_CountVect.shape[0])], \n",
    "                                    columns = train_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funtion to get counts of bigrams in each document\n",
    "def createBinaryTerms(counts):\n",
    "    x = 0.0\n",
    "    if counts>0:\n",
    "        x=1.0\n",
    "    return x\n",
    "\n",
    "train_binary_df = train_count_df.applymap(createBinaryTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of how many documents each term occurs in\n",
    "train_counts_dict = (train_binary_df.apply(sum)).to_dict()\n",
    "train_unigramsList = train_counts_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get frequency of all ref. pos. and ref. neg words\n",
    "train_vectAll = CountVectorizer(min_df=0, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running code with all words (not removing words based on frequency) \n",
    "train_CountVectAll = train_vectAll.fit_transform(train_SSsubset_reviewslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sparse dataframe using vectorizer\n",
    "train_count_df_all = pd.SparseDataFrame([pd.SparseSeries(train_CountVectAll[i].toarray().ravel()) \n",
    "                                         for i in np.arange(train_CountVectAll.shape[0])], \n",
    "                                        columns = train_vectAll.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with all term counts\n",
    "train_binary_df_all = train_count_df_all.applymap(createBinaryTerms)\n",
    "train_counts_dict_all = (train_binary_df_all.apply(sum)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates list of positive and negative reference words that appear in the training set\n",
    "posMatches_train = list(set(train_counts_dict_all.keys()).intersection(set(posMatches)))\n",
    "negMatches_train = list(set(train_counts_dict_all.keys()).intersection(set(negMatches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of how many docs the reference negative and reference positive words appear in \n",
    "# Only within training set (vs whole corpus)\n",
    "refWordsDictTrain = {}\n",
    "for x in train_counts_dict_all.keys():\n",
    "    if x in posMatches_train:\n",
    "        refWordsDictTrain[x] = train_counts_dict_all[x]\n",
    "    elif x in negMatches_train:\n",
    "        refWordsDictTrain[x] = train_counts_dict_all[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates new column with tagged review\n",
    "train_SSsubset['TaggedReview'] = train_SSsubset['reviews'].apply(tag_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(so, good), (pretty, good), (more, items), (s...\n",
       "1       [(chelsea, s), (short, rib), (tacos, green), (...\n",
       "2       [(first, time), (here.my, husband), (good, foo...\n",
       "3                           [(so, glad), (smash, burger)]\n",
       "4       [(nothing, huge), (nice, alternative), (ever, ...\n",
       "5                        [(cute, place), (tiny, parking)]\n",
       "6       [(mexican, food), (mexican, joints), (cheese, ...\n",
       "7       [(i, wish), (right, things), (very, high), (hi...\n",
       "8       [(good, restaurant), (southside, counterpart),...\n",
       "9       [(awesome, weekend), (brunch, best), (best, bl...\n",
       "10      [(italian, deli), (deli, counter), (entire, ba...\n",
       "11      [(super, l), (kong, express), (sam, woo), (3-i...\n",
       "12      [(authentic, hole), (so, make), (make, sure), ...\n",
       "13                                    [(very, impressed)]\n",
       "14      [(really, impressed), (il, posto), (mediocre, ...\n",
       "15      [(ubiquitous, gringos), (american-mexican, joi...\n",
       "16      [(bad, food), (food, bad), (bad, service), (so...\n",
       "17      [(favorite, restaurants), (north, phx), (alway...\n",
       "18           [(great, price), (very, good), (good, food)]\n",
       "19      [(ultimate, panacea), (el, norteno), (sunday, ...\n",
       "20      [(big, john), (t, forget), (hot, peppers), (le...\n",
       "21                   [(everything, great), (few, things)]\n",
       "22      [(central, phoenix), (five-0, s.), (delish, sp...\n",
       "23      [(sunday, morning), (other, time), (much, walk...\n",
       "24      [(pediatric, boards), (no-more-studying-at-cof...\n",
       "25      [(pretty, good), (not, great), (sub, par), (pr...\n",
       "26      [(amazing, pizzeria), (phenomenal, pie), (valu...\n",
       "27      [(best, bbq), (bbq, hap), (several, reason), (...\n",
       "28      [(pat, s), (first, time), (good, reviews), (re...\n",
       "29      [(totally, uninspired), (just, ridiculous), (s...\n",
       "                              ...                        \n",
       "1310    [(always, fresh), (casual, food), (decent, pri...\n",
       "1311    [(south, mountain), (well, worth), (massive, f...\n",
       "1312    [(italian, restaurant), (only, warning), (bit,...\n",
       "1313    [(fantastic, restuarant), (very, good), (good,...\n",
       "1314    [(raw, flesh), (bite, size), (as, good), (more...\n",
       "1315    [(indian, restaurant), (phoenix, worth), (othe...\n",
       "1316    [(high, school), (old, hood), (nice, staff), (...\n",
       "1317    [(few, times), (email, list), (first, time), (...\n",
       "1318                        [(very, good), (menu, items)]\n",
       "1319                                     [(pita, jungle)]\n",
       "1320    [(very, nice), (several, things), (several, gi...\n",
       "1321                       [(bad, option), (white, meat)]\n",
       "1322    [(gated, line), (delicatessen, sandwiches), (c...\n",
       "1323    [(great, food), (great, view), (different, poi...\n",
       "1324    [(big, fan), (traditional, cafeteria), (nicer,...\n",
       "1325          [(last, time), (pretty, good), (not, much)]\n",
       "1326    [(apparently, wish), (negative, scores), (gree...\n",
       "1327    [(great, place), (awesome, food), (short, rib)...\n",
       "1328    [(nice, lunch), (too, small), (not, good), (go...\n",
       "1329    [(b-day, dinner), (so, i), (everyone, s), (s, ...\n",
       "1330                                      [(fish, onion)]\n",
       "1331    [(same, people), (s, cuisine), (new, mexico), ...\n",
       "1332    [(saturday, night), (3-5, minutes), (long, tim...\n",
       "1333    [(time, i), (i, eat), (same, thing), (nothing,...\n",
       "1334    [(happy, hour), (long, day), (suns, game), (ni...\n",
       "1335    [(central, phoenix), (s, right), (central, pho...\n",
       "1336    [(mediterranean, sandwiches), (roasted, chicke...\n",
       "1337    [(seriously, unfair), (long, lines), (lines, c...\n",
       "1338    [(actual, restaurant), (outdoor, area), (outdo...\n",
       "1339    [(much, time), (lunch, time), (good, place), (...\n",
       "Name: TaggedReview, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates list of tagged bigrams\n",
    "list_of_train_bigrams = train_SSsubset['TaggedReview'].apply(get_bigrams)\n",
    "list_of_train_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('so', 'good'),\n",
       " ('pretty', 'good'),\n",
       " ('more', 'items'),\n",
       " ('so', 'good.kind'),\n",
       " ('small', 'place'),\n",
       " ('good', 'thing'),\n",
       " ('chelsea', 's'),\n",
       " ('short', 'rib'),\n",
       " ('tacos', 'green'),\n",
       " ('chile', 'burger'),\n",
       " ('salted', 'chocolate'),\n",
       " ('way', 'out.i'),\n",
       " ('out.i', 'd'),\n",
       " ('only', 'gripe'),\n",
       " ('strong', 'menu'),\n",
       " ('first', 'time'),\n",
       " ('here.my', 'husband'),\n",
       " ('good', 'food'),\n",
       " ('fried', 'onion'),\n",
       " ('red', 'pepper'),\n",
       " ('decent', 'selection'),\n",
       " ('good', 'tenderloin'),\n",
       " ('just', 'ok.'),\n",
       " ('i', 'm'),\n",
       " ('basic', 'mustard'),\n",
       " ('onion.my', 'husband'),\n",
       " ('next', 'time'),\n",
       " ('time', 'i'),\n",
       " ('i', 'll'),\n",
       " ('particular', 'day'),\n",
       " ('pretty', 'slow'),\n",
       " ('very', 'friendly'),\n",
       " ('understandable.the', 'prices'),\n",
       " ('pretty', 'reasonable'),\n",
       " ('too', 'bad'),\n",
       " ('very', 'full'),\n",
       " ('totally', 'comfortable'),\n",
       " ('pretty', 'much'),\n",
       " ('so', 'glad'),\n",
       " ('smash', 'burger'),\n",
       " ('nothing', 'huge'),\n",
       " ('nice', 'alternative'),\n",
       " ('ever', 'crap-tastic'),\n",
       " ('pita', 'jungle'),\n",
       " ('cute', 'place'),\n",
       " ('tiny', 'parking'),\n",
       " ('mexican', 'food'),\n",
       " ('mexican', 'joints'),\n",
       " ('cheese', 'burrito'),\n",
       " ('first', 'time'),\n",
       " ('valley', 'establishment'),\n",
       " ('i', 'wish'),\n",
       " ('right', 'things'),\n",
       " ('very', 'high'),\n",
       " ('high', 'quality'),\n",
       " ('next', 'time'),\n",
       " ('tad', 'bit'),\n",
       " ('bit', 'disappointed'),\n",
       " ('more', 'choices'),\n",
       " ('carne', 'asada'),\n",
       " ('mexican', 'coca-cola'),\n",
       " ('overall', 'cute'),\n",
       " ('very', 'tiny'),\n",
       " ('second', 'visit'),\n",
       " ('good', 'restaurant'),\n",
       " ('southside', 'counterpart'),\n",
       " ('green', 'jalapeno/tomatillo'),\n",
       " ('red', 'chili'),\n",
       " ('burro', 'enchilada'),\n",
       " ('enchilada', 'style'),\n",
       " ('close', '-1'),\n",
       " ('awesome', 'weekend'),\n",
       " ('brunch', 'best'),\n",
       " ('best', 'bloody'),\n",
       " ('marys', 'wonderful'),\n",
       " ('wonderful', 'staff'),\n",
       " ('really', 'good'),\n",
       " ('happy', 'hour'),\n",
       " ('italian', 'deli'),\n",
       " ('deli', 'counter'),\n",
       " ('entire', 'back'),\n",
       " ('meats', 'cheeses'),\n",
       " ('prepared', 'dishes'),\n",
       " ('baked', 'goods'),\n",
       " ('great', 'lunch'),\n",
       " ('italian', 'delicacies'),\n",
       " ('t', 'find'),\n",
       " ('find', 'anywhere'),\n",
       " ('super', 'l'),\n",
       " ('kong', 'express'),\n",
       " ('sam', 'woo'),\n",
       " ('3-in-1', 'place'),\n",
       " ('l-shaped', 'configuration'),\n",
       " ('western', 'style'),\n",
       " ('style', 'chinese'),\n",
       " ('fast-food', 'line'),\n",
       " ('more', 'authentic'),\n",
       " ('authentic', 'made-to-order'),\n",
       " ('other', 'side'),\n",
       " ('proudly', 'advertise'),\n",
       " ('chinese/asian', 'fusion'),\n",
       " ('chinese', 'restaurants'),\n",
       " ('chinese', 'restaurants'),\n",
       " ('fast-food', 'line'),\n",
       " ('authentic', 'stuff'),\n",
       " ('pork', 'roast'),\n",
       " ('roast', 'duck'),\n",
       " ('not', 'much'),\n",
       " ('pretty', 'much'),\n",
       " ('nothing', 'special'),\n",
       " ('i', 'll'),\n",
       " ('noodle', 'dishes'),\n",
       " ('steamed', 'dumplings'),\n",
       " ('good', 'flavors'),\n",
       " ('greatest', 'cuts'),\n",
       " ('taiwanese', 'style'),\n",
       " ('most', 'recent'),\n",
       " ('recent', 'visit'),\n",
       " ('beef', 'noodle'),\n",
       " ('noodle', 'soup'),\n",
       " ('wonton', 'noodle'),\n",
       " ('noodle', 'soup'),\n",
       " ('beef', 'noodle'),\n",
       " ('noodle', 'soup'),\n",
       " ('preserved', 'vegetables'),\n",
       " ('t', 'tough'),\n",
       " ('definitely', 'wasn'),\n",
       " ('t', 'tender'),\n",
       " ('lesser', 'quality'),\n",
       " ('flat', 'rice'),\n",
       " ('strong', 'flavors'),\n",
       " ('wonton', 'noodle'),\n",
       " ('noodle', 'soup'),\n",
       " ('shrimp-and-pork', 'wontons'),\n",
       " ('surprisingly', 'hard'),\n",
       " ('too', 'lean'),\n",
       " ('very', 'reasonable'),\n",
       " ('chinese', 'supermarket'),\n",
       " ('fast-food', 'counter'),\n",
       " ('fast-food', 'tables'),\n",
       " ('super', 'l'),\n",
       " ('quick', 'meal'),\n",
       " ('other', 'places'),\n",
       " ('better', 'options'),\n",
       " ('authentic', 'hole'),\n",
       " ('so', 'make'),\n",
       " ('make', 'sure'),\n",
       " ('fish', 'tacos'),\n",
       " ('cheese', 'breads'),\n",
       " ('pretty', 'cheap'),\n",
       " ('so', 'feel'),\n",
       " ('very', 'impressed'),\n",
       " ('really', 'impressed'),\n",
       " ('il', 'posto'),\n",
       " ('mediocre', 'reviews'),\n",
       " ('yelper', 'christina'),\n",
       " ('here', 'last'),\n",
       " ('last', 'night'),\n",
       " ('perfect.the', 'bartender'),\n",
       " ('really', 'friendly'),\n",
       " ('very', 'welcome'),\n",
       " ('fantastic', 'patio'),\n",
       " ('happy', 'hour'),\n",
       " ('i', 'thought'),\n",
       " ('for.our', 'group'),\n",
       " ('different', 'pizza'),\n",
       " ('happy', 'hour'),\n",
       " ('entire', 'time'),\n",
       " ('time', 'i'),\n",
       " ('delish', 'speciality'),\n",
       " ('ubiquitous', 'gringos'),\n",
       " ('american-mexican', 'joint'),\n",
       " ('good', 'mood'),\n",
       " ('next-door', 'hilton'),\n",
       " ('everyone', 'huge'),\n",
       " ('huge', 'outdoor'),\n",
       " ('s', 'time'),\n",
       " ('generally', 'happy'),\n",
       " ('copious', 'amounts'),\n",
       " ('bad', 'food'),\n",
       " ('food', 'bad'),\n",
       " ('bad', 'service'),\n",
       " ('so', 'dissapointed'),\n",
       " ('rude', 'waiter'),\n",
       " ('very', 'uncomfortable'),\n",
       " ('very', 'poor'),\n",
       " ('enhilada', 'dish'),\n",
       " ('favorite', 'restaurants'),\n",
       " ('north', 'phx'),\n",
       " ('always', 'happy'),\n",
       " ('little', 'ones'),\n",
       " ('sooo', 'good'),\n",
       " ('best', 'way'),\n",
       " ('also', 'good'),\n",
       " ('good', 'prices'),\n",
       " ('totally', 'doable'),\n",
       " ('great', 'price'),\n",
       " ('very', 'good'),\n",
       " ('good', 'food'),\n",
       " ('ultimate', 'panacea'),\n",
       " ('el', 'norteno'),\n",
       " ('sunday', 'i'),\n",
       " ('lesser', 'man'),\n",
       " ('so', 'drastic'),\n",
       " ('foolish', 'i'),\n",
       " ('already', 'intolerable'),\n",
       " ('intolerable', 'morning'),\n",
       " ('savoury', 'green'),\n",
       " ('green', 'chile'),\n",
       " ('pork', 'creamy'),\n",
       " ('creamy', 'pollo'),\n",
       " ('delicious', 'beans'),\n",
       " ('cheese', 'gallons'),\n",
       " ('mexican', 'version'),\n",
       " ('small', 'guatemalan'),\n",
       " ('guatemalan', 'oompa'),\n",
       " ('oompa', 'loompas'),\n",
       " ('little', 'hallucinatory'),\n",
       " ('same', 'look'),\n",
       " ('so', 'close'),\n",
       " ('i', 'rode'),\n",
       " ('same', 'homeless'),\n",
       " ('little', 'closet'),\n",
       " ('portly', 'hispanic'),\n",
       " ('hispanic', 'guys'),\n",
       " ('mexican', 'pizza'),\n",
       " ('carne', 'asada'),\n",
       " ('asada', 'cheese'),\n",
       " ('cheese', 'crisp'),\n",
       " ('eating', 'area'),\n",
       " ('mexican', 'food'),\n",
       " ('big', 'john'),\n",
       " ('t', 'forget'),\n",
       " ('hot', 'peppers'),\n",
       " ('lettuce', 'mayo'),\n",
       " ('standard', 'dress'),\n",
       " ('only', 'complaint'),\n",
       " ('sans', 'cheese'),\n",
       " ('vinegar', 'chips'),\n",
       " ('everything', 'great'),\n",
       " ('few', 'things'),\n",
       " ('central', 'phoenix'),\n",
       " ('five-0', 's.'),\n",
       " ('delish', 'spicy'),\n",
       " ('very', 'attentive'),\n",
       " ('happy', 'today'),\n",
       " ('good', 'friday'),\n",
       " ('romantic', 'evening'),\n",
       " ('full', 'moon'),\n",
       " ('very', 'welcome'),\n",
       " ('hawaiian', 'lobster'),\n",
       " ('creamy', 'lobster'),\n",
       " ('siracha', 'pepper'),\n",
       " ('mango-papaya', 'sauce'),\n",
       " ('late', 'nighter.later'),\n",
       " ('sweet', 'plantains'),\n",
       " ('sunday', 'morning'),\n",
       " ('other', 'time'),\n",
       " ('much', 'walk'),\n",
       " ('american', 'comfort'),\n",
       " ('southern', 'hospitality'),\n",
       " ('usually', 'a+'),\n",
       " ('so', 'nice'),\n",
       " ('favorite', 'breakfast'),\n",
       " ('real', 'blueberry'),\n",
       " ('chicken', 'fried'),\n",
       " ('fried', 'steak'),\n",
       " ('fine', 'dining'),\n",
       " ('pediatric', 'boards'),\n",
       " ('no-more-studying-at-coffee-shops-during-dinner', 'celebration'),\n",
       " ('brand', 'new'),\n",
       " ('few', 'months'),\n",
       " ('more', 'mexican'),\n",
       " ('something', 'different'),\n",
       " ('night.we', 'sat'),\n",
       " ('generous', 'booth'),\n",
       " ('back', 'corner'),\n",
       " ('wine', 'deal'),\n",
       " ('bacon-wrapped', 'scallops'),\n",
       " ('crab-stuffed', 'mushrooms'),\n",
       " ('amazingly', 'different'),\n",
       " ('magical.my', 'fianc'),\n",
       " ('roasted', 'corn'),\n",
       " ('chili', 'pepper'),\n",
       " ('roasted', 'eggplant'),\n",
       " ('like', 'buttery'),\n",
       " ('hard', 'shell'),\n",
       " ('very', 'memorable'),\n",
       " ('pretty', 'good'),\n",
       " ('not', 'great'),\n",
       " ('sub', 'par'),\n",
       " ('pretty', 'good'),\n",
       " ('nothing', 'great'),\n",
       " ('pretty', 'good'),\n",
       " ('bit', 'shuffled'),\n",
       " ('wait', 'staff'),\n",
       " ('pretty', 'long'),\n",
       " ('long', 'time'),\n",
       " ('text', 'message'),\n",
       " ('so', 'many'),\n",
       " ('good', 'restaurants'),\n",
       " ('too', 'much'),\n",
       " ('rich', 'lady'),\n",
       " ('lady', 's'),\n",
       " ('s', 'vacation'),\n",
       " ('amazing', 'pizzeria'),\n",
       " ('phenomenal', 'pie'),\n",
       " ('valuable', 'lesson'),\n",
       " ('best', 'pizzas'),\n",
       " ('vegetarian', 'options'),\n",
       " ('angel', 'pizza'),\n",
       " ('yummy', 'sausage'),\n",
       " ('companions', 'vegetarian'),\n",
       " ('vegetarian', 'orders'),\n",
       " ('vegetable', 'ingredients'),\n",
       " ('beautiful', 'medley'),\n",
       " ('garlic', 'infusions'),\n",
       " ('best', 'bbq'),\n",
       " ('bbq', 'hap'),\n",
       " ('several', 'reason'),\n",
       " ('only', 'reason'),\n",
       " ('traditional', 'beef'),\n",
       " ('odder', 'choices'),\n",
       " ('don', 't'),\n",
       " ('hap', 's'),\n",
       " ('green', 'mesquite'),\n",
       " ('green', 'mesquite'),\n",
       " ('really', 'rich'),\n",
       " ('rich', 'smoke'),\n",
       " ('rich', 'taste'),\n",
       " ('t', 'need'),\n",
       " ('thank', 'god'),\n",
       " ('good', 'compliment'),\n",
       " ('garlic', 'mashes'),\n",
       " ('very', 'flavorful'),\n",
       " ('phoenix', 'metro'),\n",
       " ('lunch', 'hour'),\n",
       " ('pat', 's'),\n",
       " ('first', 'time'),\n",
       " ('good', 'reviews'),\n",
       " ('really', 'hungry'),\n",
       " ('ny', 'style'),\n",
       " ('burnt', 'section'),\n",
       " ('thin', 'crispy'),\n",
       " ('bottom', 'squishy'),\n",
       " ('totally', 'uninspired'),\n",
       " ('just', 'ridiculous'),\n",
       " ('so', 'many'),\n",
       " ('more', 'inventive'),\n",
       " ('inventive', 'things'),\n",
       " ('bit', 'overwhelmed'),\n",
       " ('few', 'minutes'),\n",
       " ('just', 'ridiculous'),\n",
       " ('out', 'cold'),\n",
       " ('still', 'decent'),\n",
       " ('best', 'i'),\n",
       " ('nothing', 'special'),\n",
       " ('too', 'bad'),\n",
       " ('best', 'carne'),\n",
       " ('time', 'i'),\n",
       " ('i', 'm'),\n",
       " ('wine', 'superb'),\n",
       " ('usually', 'excellent'),\n",
       " ('osso', 'bucco'),\n",
       " ('long', 'time'),\n",
       " ('table', 'choice'),\n",
       " ('rude', 'tone'),\n",
       " ('few', 'months'),\n",
       " ('wait', 'staff'),\n",
       " ('.don', 't'),\n",
       " ('good', 'egg'),\n",
       " ('only', 'reason'),\n",
       " ('green', 'salad'),\n",
       " ('new', 'salad'),\n",
       " ('spilled', 'wine'),\n",
       " ('incredible', 'craving'),\n",
       " ('rosita', 's'),\n",
       " ('s', 'place'),\n",
       " ('cheese', 'burritos'),\n",
       " ('burritos', 'enchilada'),\n",
       " ('enchilada', 'style'),\n",
       " ('quality', 'authentic'),\n",
       " ('mexican', 'food'),\n",
       " ('joint', 'offers'),\n",
       " ('first', 'stop'),\n",
       " ('surly', 'wait'),\n",
       " ('worn', 'exterior.this'),\n",
       " ('exterior.this', 'place'),\n",
       " ('mexican', 'city'),\n",
       " ('other', 'locations'),\n",
       " ('favorite', 'lunch'),\n",
       " ('time', 'i'),\n",
       " ('i', 'swing'),\n",
       " ('gooey', 'cheese'),\n",
       " ('great', 'start'),\n",
       " ('enchiladas', 'suiza'),\n",
       " ('green', 'sauce-yum'),\n",
       " ('other', 'joints'),\n",
       " ('sorely', 'disappointed'),\n",
       " ('heavenly', 'good'),\n",
       " ('tacos-the', 'meat'),\n",
       " ('sure', 'anything'),\n",
       " ('go-to', 'dishes.the'),\n",
       " ('always', 'top'),\n",
       " ('top', 'notch'),\n",
       " ('lunch', 'hour'),\n",
       " ('great', 'food'),\n",
       " ('food', 'good'),\n",
       " ('atmosphere', 'good'),\n",
       " ('good', 'wine'),\n",
       " ('so', 'impressed'),\n",
       " ('huge', 'atmosphere'),\n",
       " ('sunday', 'afternoon'),\n",
       " ('packed', 'patio'),\n",
       " ('good', 'sign'),\n",
       " ('great', 'food'),\n",
       " ('cheap', 'drinks'),\n",
       " ('overly', 'gracious'),\n",
       " ('great', 'sign.i'),\n",
       " ('bloody', 'mary'),\n",
       " ('mary', 's'),\n",
       " ('nice', 'crispy'),\n",
       " ('large', 'cut'),\n",
       " ('best', 'pad'),\n",
       " ('i', 've'),\n",
       " ('here', 'last'),\n",
       " ('last', 'year'),\n",
       " ('smaller', 'thai'),\n",
       " ('friendly', 'service'),\n",
       " ('great', 'job'),\n",
       " ('tight', 'lunch'),\n",
       " ('little', 'salad'),\n",
       " ('central', 'i'),\n",
       " ('good', 'cajun'),\n",
       " ('most', 'recent'),\n",
       " ('recent', 'visit'),\n",
       " ('crayfish', 'etouffee'),\n",
       " ('very', 'good'),\n",
       " ('excellent', 'sauce'),\n",
       " ('good', 'ribs'),\n",
       " ('restaurants', 'crayfish'),\n",
       " ('crayfish', 'etouffee'),\n",
       " ('other', 'hand'),\n",
       " ('almost', 'impossible'),\n",
       " ('phoenix', 'az'),\n",
       " ('as', 'good'),\n",
       " ('i', 've'),\n",
       " ('small', 'differences'),\n",
       " ('gumbo', 'recipes'),\n",
       " ('new', 'orleans'),\n",
       " ('exceptionally', 'tasty'),\n",
       " ('turbo', 'dog'),\n",
       " ('good', 'times'),\n",
       " ('grand', 'opening'),\n",
       " ('even', 'hungry'),\n",
       " ('white', 'bean'),\n",
       " ('vegetable', 'soup'),\n",
       " ('very', 'tasty'),\n",
       " ('really', 'liked'),\n",
       " ('liked', 'it.we'),\n",
       " ('strawberry', 'raspberry'),\n",
       " ('finely', 'chopped'),\n",
       " ('chopped', 'cinnamon'),\n",
       " ('poppy', 'seed'),\n",
       " ('really', 'good.i'),\n",
       " ('good.i', 'didn'),\n",
       " ('didn', 't'),\n",
       " ('t', 'taste'),\n",
       " ('overly', 'fresh'),\n",
       " ('even', 'happier'),\n",
       " ('iced', 'tea'),\n",
       " ('slow', 'service'),\n",
       " ('t', 'bad'),\n",
       " ('not', 'awesome'),\n",
       " ('total', 'poops'),\n",
       " ('many', 'others'),\n",
       " ('best', 'fountain'),\n",
       " ('asian', 'cuisine'),\n",
       " ('so', 'many'),\n",
       " ('many', 'places'),\n",
       " ('wonderful', 'city'),\n",
       " ('asian', 'cooking'),\n",
       " ('first', 'glance'),\n",
       " ('english', 'menu'),\n",
       " ('chinese', 'menu..'),\n",
       " ('english', 'menu'),\n",
       " ('chinese', 'menu'),\n",
       " ('english', 'menu'),\n",
       " ('typical', 'stuff'),\n",
       " ('chinese', 'food'),\n",
       " ('chinese', 'menu'),\n",
       " ('good', 'stuff'),\n",
       " ('fortunately', 'helen'),\n",
       " ('helen', 'speaks'),\n",
       " ('shanghainese', 'cooking'),\n",
       " ('disappointment.of', 'course'),\n",
       " ('fish', 'soup'),\n",
       " ('veggie', 'dish'),\n",
       " ('very', 'tasty'),\n",
       " ('rice', 'noodle'),\n",
       " ('noodle', 'disks'),\n",
       " ('shredded', 'meat'),\n",
       " ('spicy', 'tendon..'),\n",
       " ('tendon..', 'yes'),\n",
       " ('cow', 'tendon'),\n",
       " ('very', 'saucy'),\n",
       " ('good', 'times'),\n",
       " ('pickled', 'veggies'),\n",
       " ('really', 'tender'),\n",
       " ('sour', 'bullshit'),\n",
       " ('light', 'rail.my'),\n",
       " ('rail.my', 'favorite'),\n",
       " ('favorite', 'item'),\n",
       " ('great', 'burgers'),\n",
       " ('central', 'i'),\n",
       " ('maizie', 's'),\n",
       " ('vegetarian', 'pizza'),\n",
       " ('extremely', 'tasty'),\n",
       " ('very', 'small'),\n",
       " ('little', 'place'),\n",
       " ('interesting', 'salads'),\n",
       " ('apps', 'main'),\n",
       " ('main', 'dishes'),\n",
       " ('not', 'sure'),\n",
       " ('second', 'visit'),\n",
       " ('white', 'bean'),\n",
       " ('apparently', 'new'),\n",
       " ('warm', 'greens'),\n",
       " ('white', 'beans'),\n",
       " ('white', 'beans'),\n",
       " ('green', 'beans'),\n",
       " ('red', 'peppers'),\n",
       " ('totally', 'vegetarian'),\n",
       " ('delicious', 'touch'),\n",
       " ('shrimp', 'risotto'),\n",
       " ('much', 'better'),\n",
       " ('web', 'site'),\n",
       " ('tiny', 'restaurant'),\n",
       " ('bad', 'thing'),\n",
       " ('local', 'restaurants'),\n",
       " ('last', 'year'),\n",
       " ('fast', 'food'),\n",
       " ('same', 'thing'),\n",
       " ('good', 'one'),\n",
       " ('healthy', 'options'),\n",
       " ('large', 'salad'),\n",
       " ('same', 'quantity'),\n",
       " ('small', 'salad'),\n",
       " ('just', 'extra'),\n",
       " ('extra', 'lettuce'),\n",
       " ('italian', 'soda'),\n",
       " ('rad', 'idea'),\n",
       " ('outdoor', 'area'),\n",
       " ('warm', 'summer'),\n",
       " ('only', 'ones'),\n",
       " ('different', 'types'),\n",
       " ('just', 'stale'),\n",
       " ('stale', 'tortilla'),\n",
       " ('positive', 'aspects'),\n",
       " ('so', 'i'),\n",
       " ('toasted', 'turkey'),\n",
       " ('so', 'sure'),\n",
       " ('enough', 'i'),\n",
       " ('great', 'sandwich'),\n",
       " ('other', 'choices'),\n",
       " ('as', 'good'),\n",
       " ('sure', 'i'),\n",
       " ('t', 'hurt'),\n",
       " ('good', 'review'),\n",
       " ('little', 'place'),\n",
       " ('bridal', 'shower'),\n",
       " ('so', 'cute'),\n",
       " ('super', 'tasty'),\n",
       " ('creamy', 'corn'),\n",
       " ('glazed', 'sausage'),\n",
       " ('really', 'good'),\n",
       " ('really', 'heinous'),\n",
       " ('i', 'glad'),\n",
       " ('so', 'lax'),\n",
       " ('more', 'attentive'),\n",
       " ('whole', 'table'),\n",
       " ('first', 'time'),\n",
       " ('self', 'serves'),\n",
       " ('other', 'drink'),\n",
       " ('drink', 'other'),\n",
       " ('jamaican', 'food'),\n",
       " ('so', 'good'),\n",
       " ('so', 'gooooood'),\n",
       " ('best', 'oxtail'),\n",
       " ('wee', 'bit'),\n",
       " ('bit', 'larger'),\n",
       " ('larger', 'meat-wise'),\n",
       " ('only', 'request'),\n",
       " ('more', 'side'),\n",
       " ('fast', 'service'),\n",
       " ('food', 'affordable'),\n",
       " ('affordable', 'experience'),\n",
       " ('glad', 'i'),\n",
       " ('very', 'glad'),\n",
       " ('glad', 'i'),\n",
       " ('lunch', 'hour'),\n",
       " ('little', 'hive'),\n",
       " ('5-15', 'minute'),\n",
       " ('few', 'brave'),\n",
       " ('enough', 'seasoning'),\n",
       " ('very', 'enjoyable'),\n",
       " ('high', 'remarks'),\n",
       " ('wait', 'staff'),\n",
       " ('very', 'pleasant'),\n",
       " ('relaxed', 'restaurant'),\n",
       " ('good', 'selection'),\n",
       " ('great', 'place'),\n",
       " ('lazy', 'saturday'),\n",
       " ('friends.while', 'i'),\n",
       " ('modern', 'tiki'),\n",
       " ('cool', 'additions'),\n",
       " ('old', 'building'),\n",
       " ('perfect', 'night'),\n",
       " ('entire', 'restaurant'),\n",
       " ('great', 'setting.i'),\n",
       " ('probably', 'lethal'),\n",
       " ('other', 'fixings'),\n",
       " ('raw', 'fish'),\n",
       " ('main', 'dishes'),\n",
       " ('t', 'happy'),\n",
       " ('black', 'beans'),\n",
       " ('very', 'nice'),\n",
       " ('delicious.for', 'dessert'),\n",
       " ('nice', 'atmosphere'),\n",
       " ('bitchin', 'place'),\n",
       " ('shreddin', 'desert'),\n",
       " ('desert', 'classic'),\n",
       " ('grail', 'material'),\n",
       " ('cajun', 'food'),\n",
       " ('nothing', 'special'),\n",
       " ('tuesday', 'night'),\n",
       " ('few', 'people'),\n",
       " ('quiet', 'blues'),\n",
       " ('electric', 'guitar'),\n",
       " ('very', 'nice'),\n",
       " ('scruffy', 'place'),\n",
       " ('basic', 'cajun'),\n",
       " ('decent', 'brews'),\n",
       " ('more', 'upscale'),\n",
       " ('upscale', 'digs'),\n",
       " ('fancier/pricier', 'fare'),\n",
       " ('pizza', 'bianco'),\n",
       " ('i', 'thought'),\n",
       " ('s', 'newest'),\n",
       " ('newest', 'restaurant'),\n",
       " ('phoenix.their', 'selection'),\n",
       " ('right', 'amount'),\n",
       " ('italian', 'dressing'),\n",
       " ('best', 'meatballs'),\n",
       " ('flourless', 'chocolate'),\n",
       " ('great', 'service'),\n",
       " ('service', 'good'),\n",
       " ('good', 'value'),\n",
       " ('awesome', 'experience'),\n",
       " ('matador-', 'wow'),\n",
       " ('exceptionally', 'good'),\n",
       " ('now', 'i'),\n",
       " ('just', 'over-priced'),\n",
       " ('over-priced', 'junk'),\n",
       " ('cooler-than-thou', 'attitude'),\n",
       " ('fifty-something', 'reviews'),\n",
       " ('place', 'most'),\n",
       " ('stars', 'i'),\n",
       " ('t', 'care'),\n",
       " ('mexican', 'cuisine'),\n",
       " ('dick', 's'),\n",
       " ('totally', 'different'),\n",
       " ('different', 'experience'),\n",
       " ('dark', 'pueblo'),\n",
       " ('american', 'rugs'),\n",
       " ('high-backed', 'booths'),\n",
       " ('great', 'deal'),\n",
       " ('shrimp', 'quesadilla'),\n",
       " ('so', 'good'),\n",
       " ('as', 'much'),\n",
       " ('last', 'week'),\n",
       " ('as', 'large'),\n",
       " ('much', 'larger'),\n",
       " ('larger', 'groups'),\n",
       " ('private', 'room'),\n",
       " ('room', 'available'),\n",
       " ('dick', 's'),\n",
       " ('extremely', 'efficient'),\n",
       " ('little', 'pricy'),\n",
       " ('totally', 'worth'),\n",
       " ('ta', 'splurge'),\n",
       " ('decent', 'breakfast/brunch'),\n",
       " ('not', 'huge'),\n",
       " ('deviled', 'egg'),\n",
       " ('russian', 'roulette'),\n",
       " ('russian', 'roulette'),\n",
       " ('small', 'bit'),\n",
       " ('not', 'sure'),\n",
       " ('relatively', 'healthy'),\n",
       " ('best', 'mac'),\n",
       " ('cheese', 'i'),\n",
       " ('long', 'time.for'),\n",
       " ('next', 'door'),\n",
       " ('old', 'school'),\n",
       " ('school', 'ice-crean'),\n",
       " ('reasonable/slightly', 'expensive'),\n",
       " ('expensive', 'side'),\n",
       " ('here', 'last'),\n",
       " ('last', 'weekend'),\n",
       " ('great', 'selection'),\n",
       " ('nice', 'breakfast'),\n",
       " ('s', 'twice'),\n",
       " ('good', 'times'),\n",
       " ('new', 'server'),\n",
       " ('menu', 'items'),\n",
       " ('funny', 'smell'),\n",
       " ('much', 'sanitation'),\n",
       " ('new', 'glasses'),\n",
       " ('few', 'months'),\n",
       " ('only', 'complaint'),\n",
       " ('quite', 'certain'),\n",
       " ('many', 'times'),\n",
       " ('absolutely', 'delicious'),\n",
       " ('little', 'slice'),\n",
       " ('potbelly', 'turkey'),\n",
       " ('turkey', 'sandwich'),\n",
       " ('yummy', 'yummy'),\n",
       " ('very', 'nice'),\n",
       " ('nice', 'introductory'),\n",
       " ('indian', 'food.for'),\n",
       " ('more', 'experienced'),\n",
       " ('indian', 'pakistani'),\n",
       " ('afghan', 'cuisine'),\n",
       " ('par', 'experience'),\n",
       " ('favorite', 'family'),\n",
       " ('extensive', 'drinks'),\n",
       " ('mixed', 'drinks'),\n",
       " ('flavored', 'mojito'),\n",
       " ('something', 'tropical'),\n",
       " ('seared', 'ahi'),\n",
       " ('new', 'york'),\n",
       " ('absolute', 'showstopper'),\n",
       " ('apollo', 's'),\n",
       " ('east', 'side'),\n",
       " ('giant', 'lambda'),\n",
       " ('first', 'time'),\n",
       " ('very', 'good'),\n",
       " ('very', 'homemade'),\n",
       " ('very', 'good'),\n",
       " ('good', 'isnt'),\n",
       " ('as', 'good'),\n",
       " ('very', 'good'),\n",
       " ('very', 'tasty'),\n",
       " ('down', 'side'),\n",
       " ('big', 'glass'),\n",
       " ('fair', 'price'),\n",
       " ('take', 'advantage'),\n",
       " ('not', 'right'),\n",
       " ('reasonable', 'price'),\n",
       " ('profitable', 'margin'),\n",
       " ('hot', 'place'),\n",
       " ('old', 'people'),\n",
       " ('old', 'ladies'),\n",
       " ('cathy', 'dolls'),\n",
       " ('new', 'pool'),\n",
       " ('old', 'dudes'),\n",
       " ('few', 'survivors'),\n",
       " ('least', 'i'),\n",
       " ('first', 'vision'),\n",
       " ('very', 'attentive'),\n",
       " ('large', 'portions'),\n",
       " ('not', 'huge'),\n",
       " ('not', 'bad'),\n",
       " ('little', 'pricey'),\n",
       " ('i', 'm'),\n",
       " ('friendly', 'waitstaff'),\n",
       " ('grilled', 'meats'),\n",
       " ('mexican', 'food'),\n",
       " ('really', 'nice'),\n",
       " ('fantastic', 'pizza'),\n",
       " ('papa', 'christos'),\n",
       " ('very', 'fresh'),\n",
       " ('fresh', 'ingredients'),\n",
       " ('very', 'friendly'),\n",
       " ('favorite', 'spots'),\n",
       " ('central', 'phoenix'),\n",
       " ('great', 'food'),\n",
       " ('food', 'great'),\n",
       " ('great', 'drinks'),\n",
       " ('reasonable', 'prices'),\n",
       " ('sweet', 'potato'),\n",
       " ('occasional', 'bloody'),\n",
       " ('bloody', 'mary'),\n",
       " ('drinks.on', 'top'),\n",
       " ('free', 'wi-fi'),\n",
       " ('central', 'corridor'),\n",
       " ('glass', 'full'),\n",
       " ('really', 'appreciate'),\n",
       " ('present', 'tense'),\n",
       " ('just', 'doesn'),\n",
       " ('t', 'fit'),\n",
       " ('touch', 'more'),\n",
       " ('more', 'masculinity'),\n",
       " ('something', 'i'),\n",
       " ('to-go', 'options'),\n",
       " ('order.the', 'yakisoba'),\n",
       " ('very', 'good'),\n",
       " ('good', 'quality'),\n",
       " ('very', 'pleased'),\n",
       " ('grilled', 'eggroll'),\n",
       " ('dreary', 'airport'),\n",
       " ('dreary', 'airport'),\n",
       " ('fresh', 'air'),\n",
       " ('blue', 'moon'),\n",
       " ('tasty', 'selection'),\n",
       " ('mexican', 'dishes'),\n",
       " ('fish', 'tacos'),\n",
       " ('very', 'good'),\n",
       " ('definite', 'repeat'),\n",
       " ('several', 'time'),\n",
       " ('sloppy', 'lasagna'),\n",
       " ('last', 'time'),\n",
       " ('only', 'time'),\n",
       " ('third', 'rip'),\n",
       " ('great', 'food'),\n",
       " ('quick', 'service'),\n",
       " ('really', 'enjoyed'),\n",
       " ('enjoyed', 'eating'),\n",
       " ('nice', 'place'),\n",
       " ('very', 'good'),\n",
       " ('fresh', 'fish'),\n",
       " ('great', 'staff'),\n",
       " ('house', 'full'),\n",
       " ('great', 'sushi'),\n",
       " ('totally', 'awesome'),\n",
       " ('first', 'time'),\n",
       " ('soup', 'last'),\n",
       " ('last', 'night'),\n",
       " ('night', 'due'),\n",
       " ('chicken', 'noodle'),\n",
       " ('noodle', 'soup'),\n",
       " ('unusual', 'touch'),\n",
       " ('white', 'meat'),\n",
       " ('poor', 'quality'),\n",
       " ('sticky', 'brown'),\n",
       " ('nice', 'guy'),\n",
       " ('live', 'band'),\n",
       " ('chinese', 'joint'),\n",
       " ('cover', 'band'),\n",
       " ('nice', 'touch'),\n",
       " ('second', 'time'),\n",
       " ('close', 'enough'),\n",
       " ('so', 'i'),\n",
       " ('mexican', 'food'),\n",
       " ('bad', 'taste'),\n",
       " ('first', 'time'),\n",
       " ('time', 'i'),\n",
       " ('big', 'burrito'),\n",
       " ('large', 'taco'),\n",
       " ('false', 'advertisement'),\n",
       " ('so', 'i'),\n",
       " ('good', 'i'),\n",
       " ('whole', 'experience'),\n",
       " ('potato', 'fries'),\n",
       " ('very', 'tasty'),\n",
       " ('too', 'expensive'),\n",
       " ('big', 'deal'),\n",
       " ('back', 'next'),\n",
       " ('next', 'time'),\n",
       " ('happy', 'hour'),\n",
       " ('great', 'time'),\n",
       " ('best', 'prices'),\n",
       " ('all-around', 'restaurant'),\n",
       " ('long', 'time'),\n",
       " ('free', 'valet'),\n",
       " ('valet', 'beautiful'),\n",
       " ('beautiful', 'landscaping'),\n",
       " ('classy', 'entrance'),\n",
       " ('modern', 'decor'),\n",
       " ('thought-out', 'ceiling'),\n",
       " ('incredibly', 'friendly'),\n",
       " ('very', 'nice'),\n",
       " ('nice', 'gesture'),\n",
       " ('indoor/outdoor', 'setup'),\n",
       " ('white', 'sangria'),\n",
       " ('green', 'flash'),\n",
       " ('pretty', 'standard'),\n",
       " ('too', 'special'),\n",
       " ('fantastic', 'wait'),\n",
       " ('very', 'positive'),\n",
       " ('perfect', 'amount'),\n",
       " ('very', 'gracious'),\n",
       " ('wait', 'staff'),\n",
       " ('unique', 'flavor'),\n",
       " ('just', 'fantastic'),\n",
       " ('very', 'flavorful'),\n",
       " ('unique', 'shape'),\n",
       " ('standard', 'tostito'),\n",
       " ('seared', 'ahi'),\n",
       " ('melt-in-your-mouth', 'tender'),\n",
       " ('smaller', 'end'),\n",
       " ('more', 'tacos'),\n",
       " ('very', 'unique'),\n",
       " ('unique', 'kale'),\n",
       " ('creamy', 'sauce'),\n",
       " ('fish', 'tacos'),\n",
       " ('so', 'good'),\n",
       " ('pretty', 'much'),\n",
       " ('entire', 'meal'),\n",
       " ('t', 'eat'),\n",
       " ('casual', 'clothes'),\n",
       " ('very', 'quality'),\n",
       " ('quality', 'food'),\n",
       " ('large', 'quantities'),\n",
       " ('wednesday', 'night'),\n",
       " ('so', 'popular'),\n",
       " ('always', 'great'),\n",
       " ('great', 'food'),\n",
       " ('consistently', 'good'),\n",
       " ('twin', 'sisters'),\n",
       " ('pretty', 'good'),\n",
       " ('good', 'burgers'),\n",
       " ('as', 'good'),\n",
       " ('good', 'burger'),\n",
       " ('turkey', 'burger'),\n",
       " ('other', 'hand'),\n",
       " ('very', 'good'),\n",
       " ('best', 'turkey'),\n",
       " ('only', 'turkey'),\n",
       " ('overall', 'zin'),\n",
       " ('worth', 'checkin'),\n",
       " ('first', 'time'),\n",
       " ('terrible', 'experience'),\n",
       " ('lunch', 'hour'),\n",
       " ('terribly', 'busy'),\n",
       " ('advertised', 'lunch'),\n",
       " ('simple', 'beef'),\n",
       " ('so', 'scarce'),\n",
       " ('absolute', 'mess'),\n",
       " ('overly', 'long'),\n",
       " ('long', 'wait'),\n",
       " ('minimal', 'amount'),\n",
       " ('dismissive', 'staff'),\n",
       " ('first', 'watch'),\n",
       " ('always', 'outstanding'),\n",
       " ('regular', 'waitresses'),\n",
       " ('so', 'nice'),\n",
       " ('always', 'good'),\n",
       " ('decent', 'breakfast'),\n",
       " ('happy', 'environment'),\n",
       " ('good', 'food-'),\n",
       " ('salmon', 'kale'),\n",
       " ('nice', 'staff'),\n",
       " ('on-site', 'ice'),\n",
       " ('better', 'dining'),\n",
       " ('unabashed', 'fan'),\n",
       " ('i', 'm'),\n",
       " ('frequent', 'traveler'),\n",
       " ('only', 'wish'),\n",
       " ('other', 'airports'),\n",
       " ('high', 'standards'),\n",
       " ('red', 'velvet'),\n",
       " ('cupcake', 'oatmeal'),\n",
       " ('oatmeal', 'butterscotch'),\n",
       " ('usual', 'everything'),\n",
       " ('high', 'quality'),\n",
       " ('original', 'location'),\n",
       " ('best', 'part'),\n",
       " ('few', 'times'),\n",
       " ('favorite', 'dish'),\n",
       " ('tasty', 'scallops'),\n",
       " ('amazing', 'ice'),\n",
       " ('gorgeous', 'restaurant'),\n",
       " ('smells', 'terrific'),\n",
       " ('romantic', 'birthday'),\n",
       " ('cozy', 'neighborhood'),\n",
       " ('good', '+'),\n",
       " ('+', 'bad'),\n",
       " ('slow-roasted', 'pork'),\n",
       " ('pork', 'green'),\n",
       " ('green', 'chile'),\n",
       " ('maple-glazed', 'carrots'),\n",
       " ('so', 'flavorful'),\n",
       " ('classic', 'lime'),\n",
       " ('very', 'good'),\n",
       " ('bit', 'sweet'),\n",
       " ('happy', 'hour'),\n",
       " ('sit-down', 'restaurant'),\n",
       " ('mixed', 'drinks'),\n",
       " ('dive', 'bar.4'),\n",
       " ('other', 'table'),\n",
       " ('big', 'disappointment'),\n",
       " ('partial', 'credit'),\n",
       " ('small', 'portion'),\n",
       " ('new', 'york'),\n",
       " ('green', 'beans'),\n",
       " ('too', 'heavy'),\n",
       " ('too', 'much'),\n",
       " ('green', 'beans'),\n",
       " ('pretty', 'bland.6'),\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning series of lists of bigrams into one list\n",
    "trainBigramsAsList = []\n",
    "def addBigrams(bigramList):\n",
    "    for i in bigramList:\n",
    "        trainBigramsAsList.append(i)\n",
    "\n",
    "list_of_train_bigrams.map(addBigrams)\n",
    "trainBigramsAsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9744"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of unique bigrams (deduping bigramsAsList)\n",
    "bigramsListTrain= list(set((trainBigramsAsList)))\n",
    "len(bigramsListTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary with how many docs the bigrams appear in within training set\n",
    "bigramDict_train={}\n",
    "bigramDict_train=bigramDict_train.fromkeys(bigramsListTrain,0)\n",
    "\n",
    "for bigram in bigramsListTrain:\n",
    "    for cell in list_of_train_bigrams:\n",
    "        if bigram in cell:\n",
    "            bigramDict_train[bigram] = bigramDict_train[bigram]+1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing bigrams that appear in less than 10 docs\n",
    "finalTrainBigramDict = {k:v for k, v in bigramDict_train.items() if v>10}\n",
    "finalTrainBigramsList = finalTrainBigramDict.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get length of bigrams list\n",
    "len(finalTrainBigramsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Prepare Test Set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull values from 'Review' column\n",
    "test_SSsubset_reviewslist = test_SSsubset['reviews'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes words that are in less than 5% and more than 98% of all docs\n",
    "test_vect = CountVectorizer(min_df=0.05, max_df=0.98, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating count matrix for testing set\n",
    "test_CountVect = test_vect.fit_transform(test_SSsubset_reviewslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converts count matrix into count dataframe\n",
    "test_count_df = pd.SparseDataFrame([pd.SparseSeries(test_CountVect[i].toarray().ravel()) \n",
    "                                    for i in np.arange(test_CountVect.shape[0])], \n",
    "                                   columns = test_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get counts of bigrams in each document\n",
    "test_binary_df = test_count_df.applymap(createBinaryTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of how many documents each term occurs in\n",
    "test_count_dict = (test_binary_df.apply(sum)).to_dict()\n",
    "testunigramsList = test_count_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get frequency of all ref. pos. and ref. neg words\n",
    "test_vectAll = CountVectorizer(min_df=0, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running code with all words (not removing words based on frequency)\n",
    "test_CountVectAll = test_vectAll.fit_transform(test_SSsubset_reviewslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sparse dataframe using vectorizer\n",
    "test_count_df_all = pd.SparseDataFrame([pd.SparseSeries(test_CountVectAll[i].toarray().ravel()) \n",
    "                                        for i in np.arange(test_CountVectAll.shape[0])], \n",
    "                                       columns = test_vectAll.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with all term counts\n",
    "test_binary_df_all = test_count_df_all.applymap(createBinaryTerms)\n",
    "test_counts_dict_all = (test_binary_df_all.apply(sum)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates list of positive and negative reference words that appear in the test set\n",
    "posMatches_test = list(set(test_counts_dict_all.keys()).intersection(set(posMatches)))\n",
    "negMatches_test = list(set(test_counts_dict_all.keys()).intersection(set(negMatches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of how many docs the reference negative and reference positive words appear in \n",
    "# Only within test set (vs whole corpus)\n",
    "refWordsDictTest = {}\n",
    "for x in test_counts_dict_all.keys():\n",
    "    if x in posMatches_test:\n",
    "        refWordsDictTest[x] = test_counts_dict_all[x]\n",
    "    elif x in negMatches_test:\n",
    "        refWordsDictTest[x] = test_counts_dict_all[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [(best, kobe), (very, trendy), (awesome, servi...\n",
       "1                                                     []\n",
       "2      [(awesome, experience), (friendly, staff), (st...\n",
       "3      [(breakfast, time), (entire, menu), (carne, as...\n",
       "4                                                     []\n",
       "5      [(damn, good), (good, burger), (very, nice), (...\n",
       "6      [(so, glad), (really, great), (really, beautif...\n",
       "7      [(first, time), (brown, lettuce), (red, lobste...\n",
       "8      [(countless, times), (urban, tiki), (simple, t...\n",
       "9      [(crazy, jim), (other, night), (very, good), (...\n",
       "10     [(different, locations), (new, york), (high, e...\n",
       "11     [(few, years), (local, i), (t, fantastic), (fo...\n",
       "12     [(completely, non-descript), (non-descript, be...\n",
       "13     [(greek, spot), (so, worth), (f*, %), (hummus,...\n",
       "14     [(i, chalk), (notable, features), (near, future)]\n",
       "15     [(only, sign), (additional, parking), (casual,...\n",
       "16                                        [(very, good)]\n",
       "17     [(so, close), (good, thing), (flavor, otherwis...\n",
       "18     [(very, hot), (huge, portion), (chicken, parma...\n",
       "19     [(eddie, lopez), (gale, s), (west, side), (nex...\n",
       "20     [(zucca, chips), (great, snack), (next, time),...\n",
       "21     [(best, friend), (young, lady), (last, time), ...\n",
       "22     [(old, school), (very, good), (good, way.accor...\n",
       "23                                        [(hard, rock)]\n",
       "24     [(gotten, delivery), (complimentary, touch), (...\n",
       "25     [(good, buzz), (old, hood), (little, burger), ...\n",
       "26     [(restaurants, second), (second, chances), (se...\n",
       "27     [(so, happy), (happy, pita), (norterra, shops)...\n",
       "28     [(new, year), (year, s), (s, day), (pet, peeve...\n",
       "29     [(free, ribs), (extra, push), (new, spot), (ce...\n",
       "                             ...                        \n",
       "630    [(time, i), (i, order), (sour, cream), (key, w...\n",
       "631    [(normal, lunch), (foods, other), (other, cowo...\n",
       "632    [(very, first), (first, time), (happy, hour), ...\n",
       "633    [(cool, neighborhood), (good, food), (white, g...\n",
       "634    [(real, deal), (local, greek), (big, fat), (fr...\n",
       "635    [(very, good), (somewhat, limited), (wine, lis...\n",
       "636    [(first, experience), (total, letdown.1), (hal...\n",
       "637    [(past, saturday), (perfect, opportunity), (hi...\n",
       "638    [(red, chili), (red, chili), (low, quality), (...\n",
       "639    [(great, burgers), (animal, style), (bad, thin...\n",
       "640    [(good, decor), (friendly, service), (amazing,...\n",
       "641    [(chinese, restuarant), (i, ve), (chinese, res...\n",
       "642    [(america, s), (converted, house), (upper, wal...\n",
       "643                                       [(cold, soba)]\n",
       "644    [(milk, coffee), (eggplant, parmesean), (somet...\n",
       "645    [(glorified, bar), (few, bloody), (bloody, mar...\n",
       "646    [(full, disclosure), (shockingly, cruel), (cru...\n",
       "647                                                   []\n",
       "648    [(arcadia, area), (arcadian, eateries), (acaci...\n",
       "649    [(fresh, airport), (airport, mexican-like), (m...\n",
       "650    [(meh, oriental), (oriental, express), (vegeta...\n",
       "651    [(i, m), (m, happy), (happy, hour), (3-7, ever...\n",
       "652    [(tropical, style), (fresh, flavors), (fresh, ...\n",
       "653    [(i, don), (few, times), (pretty, good), (good...\n",
       "654    [(long, hiatus), (as, good), (underground, men...\n",
       "655    [(burrito, nachos), (chicken, steak), (differe...\n",
       "656    [(place.the, vegetarian), (vegetarian, pho), (...\n",
       "657                                                   []\n",
       "658    [(i, hate), (empty, table), (so, i), (big, dea...\n",
       "659    [(fancy, mac), (sloppy, baby), (sub-par, revie...\n",
       "Name: TaggedReview, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates new column with tokenized and tagged review\n",
    "test_SSsubset['TaggedReview'] = test_SSsubset['reviews'].apply(tag_review)\n",
    "list_of_test_bigrams = test_SSsubset['TaggedReview'].apply(get_bigrams)\n",
    "list_of_test_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('best', 'kobe'),\n",
       " ('very', 'trendy'),\n",
       " ('awesome', 'service'),\n",
       " ('awesome', 'experience'),\n",
       " ('friendly', 'staff'),\n",
       " ('staff', 'great'),\n",
       " ('great', 'value'),\n",
       " ('quick', 'service'),\n",
       " ('fantastically', 'nummy'),\n",
       " ('breakfast', 'time'),\n",
       " ('entire', 'menu'),\n",
       " ('carne', 'asada'),\n",
       " ('chicken', 'soft'),\n",
       " ('soft', 'taco'),\n",
       " ('pretty', 'tasteless'),\n",
       " ('shredded', 'beef'),\n",
       " ('iceberg', 'lettuce'),\n",
       " ('not', 'super'),\n",
       " ('super', 'interesting'),\n",
       " ('hot', 'sauce'),\n",
       " ('enough', 'umph'),\n",
       " ('damn', 'good'),\n",
       " ('good', 'burger'),\n",
       " ('very', 'nice'),\n",
       " ('nice', 'place'),\n",
       " ('so', 'glad'),\n",
       " ('really', 'great'),\n",
       " ('really', 'beautiful'),\n",
       " ('open', 'spaces'),\n",
       " ('open', 'kitchens'),\n",
       " ('small', 'touches'),\n",
       " ('large', 'community'),\n",
       " ('great', 'atmosphere'),\n",
       " ('communal', 'dining'),\n",
       " ('first', 'time'),\n",
       " ('so', 'good'),\n",
       " ('favorite', 'appetizers'),\n",
       " ('dinner', 'next'),\n",
       " ('next', 'time'),\n",
       " ('time', 'i'),\n",
       " ('i', 'm'),\n",
       " ('flavor', 'dumplings'),\n",
       " ('perfect', 'i'),\n",
       " ('little', 'more'),\n",
       " ('more', 'chicken'),\n",
       " ('warmer', 'imo'),\n",
       " ('whipped', 'peanut'),\n",
       " ('t', 'taste'),\n",
       " ('chocolate', 'next'),\n",
       " ('next', 'time'),\n",
       " ('first', 'time'),\n",
       " ('brown', 'lettuce'),\n",
       " ('red', 'lobster'),\n",
       " ('ready', 'mde'),\n",
       " ('i', 'm'),\n",
       " ('fresh', 'greens'),\n",
       " ('fri', 'night'),\n",
       " ('very', 'clean'),\n",
       " ('well', 'lit'),\n",
       " ('lit', 'place'),\n",
       " ('countless', 'times'),\n",
       " ('urban', 'tiki'),\n",
       " ('simple', 'today'),\n",
       " ('first', 'time'),\n",
       " ('mellow', 'tuesday'),\n",
       " ('great', 'pride'),\n",
       " ('key', 'element'),\n",
       " ('whole', 'place'),\n",
       " ('mini', 'tour'),\n",
       " ('creative', 'design'),\n",
       " ('later', 'visit'),\n",
       " ('jumpin', 'late'),\n",
       " ('late', 'hours'),\n",
       " ('same', 'excitemnet'),\n",
       " ('entire', 'staff'),\n",
       " ('very', 'good'),\n",
       " ('yet', 'creative'),\n",
       " ('bad', 'word'),\n",
       " ('warm', 'response'),\n",
       " ('first', 'reason'),\n",
       " ('original', 'design'),\n",
       " ('mini', 'tour'),\n",
       " ('amazing', 'glass'),\n",
       " ('huge', 'wood'),\n",
       " ('wood', 'common'),\n",
       " ('common', 'tables'),\n",
       " ('stellar', 'use'),\n",
       " ('various', 'materials'),\n",
       " ('local', 'downtown-istes'),\n",
       " ('ball', 'game'),\n",
       " ('crazy', 'jim'),\n",
       " ('other', 'night'),\n",
       " ('very', 'good'),\n",
       " ('softest', 'fluffiest'),\n",
       " ('almost', 'like'),\n",
       " ('indian', 'fry'),\n",
       " ('only', 'complaint'),\n",
       " ('crazy', 'jim'),\n",
       " ('first', 'date'),\n",
       " ('interesting', 'dichotomy'),\n",
       " ('private', 'dinner'),\n",
       " ('anyway', 'i'),\n",
       " ('cj', 's'),\n",
       " ('low', 'key'),\n",
       " ('different', 'locations'),\n",
       " ('new', 'york'),\n",
       " ('high', 'expectations'),\n",
       " ('other', 'locations'),\n",
       " ('very', 'good.this'),\n",
       " ('good.this', 'location'),\n",
       " ('major', 'complaint.the'),\n",
       " ('headed', 'monster'),\n",
       " ('typical', 'roy'),\n",
       " ('very', 'good'),\n",
       " ('bit', 'small'),\n",
       " ('wife', 's'),\n",
       " ('s', 'scallops'),\n",
       " ('tad', 'bit'),\n",
       " ('too', 'overcooked.we'),\n",
       " ('perfect', 'weather'),\n",
       " ('very', 'poor'),\n",
       " ('little', 'flashlights'),\n",
       " ('by.the', 'major'),\n",
       " ('major', 'complaint'),\n",
       " ('new', 'york'),\n",
       " ('san', 'francisco'),\n",
       " ('not', 'cool'),\n",
       " ('marriott', 'hotel'),\n",
       " ('definitely', 'recommend'),\n",
       " ('few', 'years'),\n",
       " ('local', 'i'),\n",
       " ('t', 'fantastic'),\n",
       " ('former', 'south-easterner'),\n",
       " ('warm', 'fuzzy'),\n",
       " ('completely', 'non-descript'),\n",
       " ('non-descript', 'berto'),\n",
       " ('outstanding', 'food'),\n",
       " ('happy', 'day'),\n",
       " ('day', 'happy'),\n",
       " ('happy', 'kids'),\n",
       " ('few', 'times'),\n",
       " ('really', 'good'),\n",
       " ('really', 'reliable'),\n",
       " ('old', 'school'),\n",
       " ('school', 'mexican'),\n",
       " ('mexican', 'food'),\n",
       " ('greek', 'spot'),\n",
       " ('so', 'worth'),\n",
       " ('f*', '%'),\n",
       " ('hummus', 'greek'),\n",
       " ('greek', 'fries'),\n",
       " ('just', 'fantastic'),\n",
       " ('lot', 'o'),\n",
       " ('o', 'food'),\n",
       " ('very', 'nice'),\n",
       " ('i', 'chalk'),\n",
       " ('notable', 'features'),\n",
       " ('near', 'future'),\n",
       " ('only', 'sign'),\n",
       " ('additional', 'parking'),\n",
       " ('casual', 'place'),\n",
       " ('small', 'wood'),\n",
       " ('wonderful', 'smell'),\n",
       " ('bloody', 'mary'),\n",
       " ('mary', 's'),\n",
       " ('attentive', 'staff'),\n",
       " ('eggs', 'benedict'),\n",
       " ('hot', 'plates'),\n",
       " ('breakfast', 'warm'),\n",
       " ('warm', 'til'),\n",
       " ('highly', 'recommend.can'),\n",
       " ('recommend.can', 't'),\n",
       " ('very', 'good'),\n",
       " ('so', 'close'),\n",
       " ('good', 'thing'),\n",
       " ('flavor', 'otherwise'),\n",
       " ('affliction', 't-shirt'),\n",
       " ('next', 'table'),\n",
       " ('durant', 's'),\n",
       " ('so', 'classy'),\n",
       " ('t', 'need'),\n",
       " ('again', 'i'),\n",
       " ('i', 'assure'),\n",
       " ('very', 'good'),\n",
       " ('chinese', 'restaurant'),\n",
       " ('red', 'booths'),\n",
       " ('great', 'wall'),\n",
       " ('plain', 'view'),\n",
       " ('personal', 'version'),\n",
       " ('half', 'belvedere'),\n",
       " ('half', 'sapphire'),\n",
       " ('perfectly', 'mixed'),\n",
       " ('substantial', 'memory'),\n",
       " ('didn', 't'),\n",
       " ('wine', 'selection'),\n",
       " ('relatively', 'reasonable'),\n",
       " ('bone-in', 'ribeye'),\n",
       " ('fantastic', 'flavor'),\n",
       " ('much', 'larger'),\n",
       " ('small', 'fixes'),\n",
       " ('fixes', 'durant'),\n",
       " ('durant', 's'),\n",
       " ('unfortunately', 'i'),\n",
       " ('worthy', 'trip'),\n",
       " ('very', 'hot'),\n",
       " ('huge', 'portion'),\n",
       " ('chicken', 'parmasian'),\n",
       " ('very', 'little'),\n",
       " ('little', 'onions'),\n",
       " ('french', 'onion'),\n",
       " ('couple', 'months'),\n",
       " ('very', 'consistent'),\n",
       " ('eddie', 'lopez'),\n",
       " ('gale', 's'),\n",
       " ('west', 'side'),\n",
       " ('next', 'visit'),\n",
       " ('zucca', 'chips'),\n",
       " ('great', 'snack'),\n",
       " ('next', 'time'),\n",
       " ('so', 'yummy'),\n",
       " ('just', 'wish'),\n",
       " ('har', 'har'),\n",
       " ('not', 'bad'),\n",
       " ('bad', 'i'),\n",
       " ('so', 'i'),\n",
       " ('other', 'friend'),\n",
       " ('really', 'good'),\n",
       " ('too', 'much'),\n",
       " ('much', 'fat'),\n",
       " ('open', 'kitchen'),\n",
       " ('hot', 'bartender'),\n",
       " ('just', 'saw'),\n",
       " ('very', 'short'),\n",
       " ('short', 'guys'),\n",
       " ('best', 'friend'),\n",
       " ('young', 'lady'),\n",
       " ('last', 'time'),\n",
       " ('white', 'watch'),\n",
       " ('first', 'time'),\n",
       " ('so', 'happy'),\n",
       " ('free', 'desserts.now'),\n",
       " ('awesome.the', 'desserts'),\n",
       " ('second', 'one'),\n",
       " ('baked', 'chocolate'),\n",
       " ('whole', 'meal'),\n",
       " ('good', 'work'),\n",
       " ('old', 'school'),\n",
       " ('very', 'good'),\n",
       " ('good', 'way.according'),\n",
       " ('best', 'steakhouses'),\n",
       " ('glad', 'i'),\n",
       " ('prime', 'rib'),\n",
       " ('select=uhk-7dquey6biqk7i8zz7g', ']'),\n",
       " ('prime', 'rib'),\n",
       " ('tender', 'extra'),\n",
       " ('extra', 'au'),\n",
       " ('bit', 'overdone'),\n",
       " ('overdone', 'i'),\n",
       " ('hard', 'rock'),\n",
       " ('gotten', 'delivery'),\n",
       " ('complimentary', 'touch'),\n",
       " ('pretty', 'good'),\n",
       " ('something', 'bad'),\n",
       " ('sunday', 'night'),\n",
       " ('good', 'buzz'),\n",
       " ('old', 'hood'),\n",
       " ('little', 'burger'),\n",
       " ('great', 'hamburgers'),\n",
       " ('delicious', 'fries'),\n",
       " ('three-patty', 'monstrosity'),\n",
       " ('10-inch', 'sub'),\n",
       " ('classic', 'joint'),\n",
       " ('joint', 'legendary'),\n",
       " ('restaurants', 'second'),\n",
       " ('second', 'chances'),\n",
       " ('second', 'time'),\n",
       " ('as', 'bad'),\n",
       " ('not', 'friendly'),\n",
       " ('apparent', 'ownership'),\n",
       " ('new', 'staff'),\n",
       " ('t', 'horrible'),\n",
       " ('so', 'happy'),\n",
       " ('happy', 'pita'),\n",
       " ('norterra', 'shops'),\n",
       " ('nice', 'area'),\n",
       " ('usually', 'good'),\n",
       " ('always', 'fresh'),\n",
       " ('happy', 'hour'),\n",
       " ('small', 'plates'),\n",
       " ('garlic', 'chicken'),\n",
       " ('weird', 'taste'),\n",
       " ('pita', 'jungle'),\n",
       " ('healthy', 'food'),\n",
       " ('new', 'year'),\n",
       " ('year', 's'),\n",
       " ('s', 'day'),\n",
       " ('pet', 'peeve'),\n",
       " ('simply', 'gorgeous'),\n",
       " ('few', 'things'),\n",
       " ('very', 'good'),\n",
       " ('lgo', 'menu'),\n",
       " ('pretty', 'cool'),\n",
       " ('free', 'ribs'),\n",
       " ('extra', 'push'),\n",
       " ('new', 'spot'),\n",
       " ('central', 'i'),\n",
       " ('unassuming', 'building'),\n",
       " ('few', 'parking'),\n",
       " ('small', 'lot'),\n",
       " ('up', 'i'),\n",
       " ('great', 'place'),\n",
       " ('great', 'ambiance'),\n",
       " ('great', 'group'),\n",
       " ('eclectic', 'servers'),\n",
       " ('mmm', 'good'),\n",
       " ('plain', 'good'),\n",
       " ('chopped', 'kale'),\n",
       " ('really', 'fresh'),\n",
       " ('other', 'stuff'),\n",
       " ('very', 'nice'),\n",
       " ('more', 'deliciousness'),\n",
       " ('maybe', 'i'),\n",
       " ('enough', 'room'),\n",
       " ('churn', 'next'),\n",
       " ('next', 'door'),\n",
       " ('so', 'good'),\n",
       " ('so', 'much'),\n",
       " ('much', 'food'),\n",
       " ('little', 'bit'),\n",
       " ('definitely', 'worth'),\n",
       " ('best', 'restaurant'),\n",
       " ('consistently', 'good'),\n",
       " ('indian', 'food'),\n",
       " ('garlic', 'naan'),\n",
       " ('always', 'attentive'),\n",
       " ('same', 'caliber'),\n",
       " ('fresh', 'hot'),\n",
       " ('pedestrian', 'lunch'),\n",
       " ('house', 'special'),\n",
       " ('special', 'chicken'),\n",
       " ('well', 'pedestrian'),\n",
       " ('not', 'nasty'),\n",
       " ('different', 'restaurant'),\n",
       " ('less', 'flamboyant'),\n",
       " ('lunch', 'special'),\n",
       " ('special', 'menu'),\n",
       " ('i', 'guess'),\n",
       " ('today', 's'),\n",
       " ('s', 'dish'),\n",
       " ('house', 'special'),\n",
       " ('special', 'chicken'),\n",
       " ('battered', 'chicken'),\n",
       " ('good', 'taste'),\n",
       " ('very', 'eventful'),\n",
       " ('eventful', 'taste'),\n",
       " ('i', 'guess'),\n",
       " ('high', 'regards'),\n",
       " ('lackluster', 'dishes'),\n",
       " ('lunch', 'combos'),\n",
       " ('only', 'good'),\n",
       " ('so', 'i'),\n",
       " ('i', 'guess'),\n",
       " ('indo/pak', 'restaurants.everything'),\n",
       " ('tiny', 'notch'),\n",
       " ('indian-restaurant', 'fare'),\n",
       " ('important', 'client'),\n",
       " ('not', 'fancy-schmancy'),\n",
       " ('fancy-schmancy', 'enough.once'),\n",
       " ('white', 'picket'),\n",
       " ('nothing', 'extraordinary'),\n",
       " ('indian', 'food'),\n",
       " ('safe', 'bet'),\n",
       " ('very', 'cheap'),\n",
       " ('very', 'happy'),\n",
       " ('main', 'meal'),\n",
       " ('meal', 'fish'),\n",
       " ('fish', 'tacos'),\n",
       " ('very', 'good'),\n",
       " ('full', 'service'),\n",
       " ('sofa', 'sheet'),\n",
       " ('average', 'ratings'),\n",
       " ('slight', 'hipster'),\n",
       " ('rather', 'comfortable'),\n",
       " ('little', 'bit'),\n",
       " ('too', 'much'),\n",
       " ('much', 'mayo'),\n",
       " ('too', 'much'),\n",
       " ('kale', 'salad'),\n",
       " ('spensive', 'ones'),\n",
       " ('roasted', 'jalapeno'),\n",
       " ('pairing', 'i'),\n",
       " ('very', 'good'),\n",
       " ('fresh', 'pepper'),\n",
       " ('pretty', 'simple'),\n",
       " ('simple', 'coffee'),\n",
       " ('sweet', 'coffee'),\n",
       " ('awfully', 'cozy'),\n",
       " ('great', 'place'),\n",
       " ('so', 'i'),\n",
       " ('many', 'sticks'),\n",
       " ('pretty', 'tasty'),\n",
       " ('few', 'minutes'),\n",
       " ('not', 'sure'),\n",
       " ('tvs', 'noise'),\n",
       " ('roaming', 'staff'),\n",
       " ('mostly', 'downhill'),\n",
       " ('other', 'food'),\n",
       " ('previous', 'diner'),\n",
       " ('too', 'expensive'),\n",
       " ('up', 'ala'),\n",
       " ('ala', 'carte'),\n",
       " ('mexican', 'music'),\n",
       " ('pretty', 'good'),\n",
       " ('only', 'thing'),\n",
       " ('pretty', 'decent'),\n",
       " ('very', 'prompt'),\n",
       " ('other', 'locations'),\n",
       " ('other', 'dishes'),\n",
       " ('more', '*exciting*'),\n",
       " ('standard', 'fare'),\n",
       " ('i', 'ate'),\n",
       " ('pleasant', 'experience'),\n",
       " ('i', 'guess'),\n",
       " ('kirin', 'wok'),\n",
       " ('private', 'catering'),\n",
       " ('very', 'attentive'),\n",
       " ('exceptional', 'service'),\n",
       " ('tofu', 'steaks'),\n",
       " ('vegetable', 'tempura'),\n",
       " ('pretty', 'good'),\n",
       " ('kitsune', 'udon'),\n",
       " ('japanese', 'noodle'),\n",
       " ('other', 'reviewers'),\n",
       " ('great', 'service'),\n",
       " ('dear', 'god'),\n",
       " ('favorite', 'things'),\n",
       " ('favorite', 'tacos'),\n",
       " ('isn', 't'),\n",
       " ('enough', 'good'),\n",
       " ('good', 'things'),\n",
       " ('slight', 'disappointment'),\n",
       " ('other', 'custom'),\n",
       " ('big', 'fan'),\n",
       " ('other', 'options'),\n",
       " ('options', 'available'),\n",
       " ('fresh', 'cilantro'),\n",
       " ('most', 'boring'),\n",
       " ('boring', 'salsa'),\n",
       " ('little', 'cilantro'),\n",
       " ('new', 'life'),\n",
       " ('sorry', 'tam'),\n",
       " ('cheese', 'crisp'),\n",
       " ('so', 'good'),\n",
       " ('tacos', 'haunt'),\n",
       " ('downtown', 'phoenix'),\n",
       " ('phoenix', 'area'),\n",
       " ('good', 'excuse'),\n",
       " ('well', 'seasoned'),\n",
       " ('green', 'chili'),\n",
       " ('green', 'chili'),\n",
       " ('very', 'tasty'),\n",
       " ('very', 'good'),\n",
       " ('first', 'visit'),\n",
       " ('next', 'time'),\n",
       " ('margarita', 's'),\n",
       " ('t', 'trendy'),\n",
       " ('t', 'fancy'),\n",
       " ('t', 'famous'),\n",
       " ('just', 'plain'),\n",
       " ('plain', 'good.i'),\n",
       " ('red', 'devil'),\n",
       " ('too', 'much'),\n",
       " ('much', 'pizza'),\n",
       " ('next', 'day'),\n",
       " ('perfectly', 'normal'),\n",
       " ('consistently', 'friendly'),\n",
       " ('menu', 'suggestions'),\n",
       " ('wide', 'variety'),\n",
       " ('main', 'dishes'),\n",
       " ('favorite', 'artichoke'),\n",
       " ('italian', 'dressing'),\n",
       " ('instant', 'breadsticks'),\n",
       " ('pizza', 'red'),\n",
       " ('red', 'devil'),\n",
       " ('yet', 'doughy'),\n",
       " ('doughy', 'crust'),\n",
       " ('tangy', 'sauce'),\n",
       " ('evenly-distributed', 'toppings'),\n",
       " ('favorite', 'pepperoni'),\n",
       " ('tastes', 'wonderful'),\n",
       " ('next', 'day'),\n",
       " ('typically', 'drink'),\n",
       " ('drink', 'specials'),\n",
       " ('pizza', 'big'),\n",
       " ('awesome', 'lunch'),\n",
       " ('recent', 'date'),\n",
       " ('too', 'hot'),\n",
       " ('little', 'dipping'),\n",
       " ('turkey', 'burger'),\n",
       " ('so', 'much'),\n",
       " ('special', 'burger'),\n",
       " ('milkshake.the', 'burger'),\n",
       " ('very', 'good'),\n",
       " ('definitely', 'good'),\n",
       " ('not', 'great'),\n",
       " ('oh', 'man'),\n",
       " ('2-star', 'rating'),\n",
       " ('last', 'night'),\n",
       " ('old', 'school'),\n",
       " ('terrible', 'service'),\n",
       " ('great', 'lunch'),\n",
       " ('few', 'months'),\n",
       " ('durant', 's'),\n",
       " ('comfy', 'booths'),\n",
       " ('clad', 'staff'),\n",
       " ('instant', 'time'),\n",
       " ('incredible', 'dining'),\n",
       " ('poor', 'rating'),\n",
       " ('prix-fixe', 'menu'),\n",
       " ('perhaps.our', 'time'),\n",
       " ('best', 'part'),\n",
       " ('few', 'cocktails'),\n",
       " ('garlic', 'bread'),\n",
       " ('warm', 'cheesy'),\n",
       " ('promise.the', 'limited'),\n",
       " ('limited', 'menu'),\n",
       " ('first', 'course'),\n",
       " ('main', 'course'),\n",
       " ('pot', 'roast'),\n",
       " ('small', 'steak'),\n",
       " ('chicken', 'fried'),\n",
       " ('fried', 'steak'),\n",
       " ('t.', 'honey'),\n",
       " ('old', 'school'),\n",
       " ('regular', 'menu'),\n",
       " ('fish', 'option'),\n",
       " ('good', 'conversation'),\n",
       " ('main', 'course'),\n",
       " ('bit', 'surprised'),\n",
       " ('fried', 'steak'),\n",
       " ('gravy-esque', 'goo'),\n",
       " ('garlic', 'mashers'),\n",
       " ('lemon', 'caper'),\n",
       " ('light', 'sauce'),\n",
       " ('fried', 'fish'),\n",
       " ('not', 'good'),\n",
       " ('fried', 'steak'),\n",
       " ('not', 'good'),\n",
       " ('appropriate', 'level'),\n",
       " ('whole', 'experience'),\n",
       " ('durant', 's'),\n",
       " ('big', 'juicy'),\n",
       " ('7-6-12.', 'i'),\n",
       " ('almost', 'full'),\n",
       " ('good', 'sign'),\n",
       " ('few', 'minutes'),\n",
       " ('main', 'thoroughfare'),\n",
       " ('huge', 'problem'),\n",
       " ('more', 'bread'),\n",
       " ('very', 'disappointing'),\n",
       " ('disappointing', 'meal'),\n",
       " ('italian', 'chain'),\n",
       " ('fish', 'tacos'),\n",
       " ('well', 'prepared'),\n",
       " ('also', 'killer.then'),\n",
       " ('killer.then', 'dinner'),\n",
       " ('spiciest', 'dish'),\n",
       " ('long', 'time'),\n",
       " ('hottest', 'curry'),\n",
       " ('obviously', 'didn'),\n",
       " ('didn', 't'),\n",
       " ('due', 'warning'),\n",
       " ('regular', 'dim-sum-er'),\n",
       " ('great', 'wal'),\n",
       " ('also-', 'fun'),\n",
       " ('vietnamese', 'weddings'),\n",
       " ('great', 'wall'),\n",
       " ('so', 'great'),\n",
       " ('great', 'wall'),\n",
       " ('chinese', 'restaurant'),\n",
       " ('important', 'part'),\n",
       " ('wedding', 'culture'),\n",
       " ('great', 'wall'),\n",
       " ('great', 'wall'),\n",
       " ('older', 'siblings'),\n",
       " ('great', 'wall'),\n",
       " ('bit', 'sentimental'),\n",
       " ('love', 'great'),\n",
       " ('great', 'wall'),\n",
       " ('only', 'place'),\n",
       " ('great', 'wall'),\n",
       " ('now', 'nice'),\n",
       " ('hopefully', 'stay'),\n",
       " ('only', 'staff'),\n",
       " ('tall', 'man'),\n",
       " ('chinese', 'brotha'),\n",
       " ('not', 'bad'),\n",
       " ('though-', 'maybe'),\n",
       " ('dimsum', 'line'),\n",
       " ('so', 'come'),\n",
       " ('really', 'don'),\n",
       " ('don', 't'),\n",
       " ('much', 'complaints'),\n",
       " ('occasionally', 'long'),\n",
       " ('long', 'wait'),\n",
       " ('dim', 'sum'),\n",
       " ('good', 'food'),\n",
       " ('inexpensive', 'price'),\n",
       " ('crazy', 'oh'),\n",
       " ('so', 'many'),\n",
       " ('many', 'things'),\n",
       " ('time', 'i'),\n",
       " ('blitzed', 'crazy'),\n",
       " ('live', 'funk'),\n",
       " ('so', 'good'),\n",
       " ('good', 'funk'),\n",
       " ('crazy', 'techno'),\n",
       " ('snob', 'boob'),\n",
       " ('job', 'chick'),\n",
       " ('chick', 'dumb-as-rocks'),\n",
       " ('ambiguous', 'raver'),\n",
       " ('last', 'time'),\n",
       " ('black', 'patent'),\n",
       " ('freaky-di', 'bondage'),\n",
       " ('black', 'eyeliner'),\n",
       " ('least', 'i'),\n",
       " ('loathing-type', 'creatures'),\n",
       " ('german', 'beers'),\n",
       " ('awesomely', 'large'),\n",
       " ('large', 'glasses'),\n",
       " ('decent', 'alcohol'),\n",
       " ('selection', 'large'),\n",
       " ('large', 'dance'),\n",
       " ('floor', 'good'),\n",
       " ('good', 'music'),\n",
       " ('really', 'great'),\n",
       " ('great', 'owner'),\n",
       " ('someone', 'special'),\n",
       " ('pitch-black', 'mysterious'),\n",
       " ('mysterious', 'tables'),\n",
       " ('restaurant', 'area'),\n",
       " ('red', 'exit'),\n",
       " ('other', 'savor'),\n",
       " ('absolute', 'zen'),\n",
       " ('line-food', 'excellent'),\n",
       " ('mexican', 'i'),\n",
       " ('soda', 'machines-the'),\n",
       " ('machines-the', 'dispenser'),\n",
       " ('anyway', 'excellent'),\n",
       " ('excellent', 'food'),\n",
       " ('solid', 'tasty'),\n",
       " ('not', 'greasy'),\n",
       " ('big', 'plus'),\n",
       " ('yelps', 'best'),\n",
       " ('lenny', 's'),\n",
       " ('lenny', 'burger'),\n",
       " ('lemon', 'juice'),\n",
       " ('bad.i', 'dont'),\n",
       " ('mexican', 'food'),\n",
       " ('contemporary', 'food'),\n",
       " ('all-you-can-eat', 'buffet'),\n",
       " ('good', 'idea'),\n",
       " ('new', 'things'),\n",
       " ('matar', 'paneer'),\n",
       " ('classy', 'form'),\n",
       " ('aloo', 'mattar'),\n",
       " ('perfect', 'amount'),\n",
       " ('immense', 'amount'),\n",
       " ('garlic', 'naan'),\n",
       " ('bad', 'news'),\n",
       " ('not', 'fifteen'),\n",
       " ('fifteen', 'minutes'),\n",
       " ('first', 'plates'),\n",
       " ('small', 'ladies'),\n",
       " ('overfilled', 'plate..'),\n",
       " ('aloo', 'mattar'),\n",
       " ('first', 'time'),\n",
       " ('long', 'time'),\n",
       " ('dr', 'service'),\n",
       " ('didn', 't'),\n",
       " ('fifteen', 'minutes'),\n",
       " ('little', 'lagging'),\n",
       " ('so', 'much'),\n",
       " ('dish', 'chicken'),\n",
       " ('next', 'time'),\n",
       " ('sushi', 'bar'),\n",
       " ('great', 'menu'),\n",
       " ('fast', 'food'),\n",
       " ('everything', 'on-site'),\n",
       " ('downtown', 'spot'),\n",
       " ('early', 'dinner'),\n",
       " ('first', 'stop'),\n",
       " ('nothing', 'super-special'),\n",
       " ('just', 'good'),\n",
       " ('good', 'food'),\n",
       " ('food', 'good'),\n",
       " ('good', 'service'),\n",
       " ('service', 'excellent'),\n",
       " ('excellent', 'prices'),\n",
       " ('local', 'spot'),\n",
       " ('free', 'chips'),\n",
       " ('delish.only', 'weird'),\n",
       " ('weird', 'thing'),\n",
       " ('different', 'kinds'),\n",
       " ('complete', 'dinners'),\n",
       " ('always', 'busy'),\n",
       " ('so', 'long'),\n",
       " ('so', 'anyway'),\n",
       " ('anyway', 'suffice'),\n",
       " ('friday', 'evening'),\n",
       " ('oldest', 'daughter'),\n",
       " ('not', 'sure'),\n",
       " ('more', 'things'),\n",
       " ('things', 'in-between'),\n",
       " ('nearly', 'full'),\n",
       " ('full', 'lot'),\n",
       " ('shiny', 'metal'),\n",
       " ('fast-industrial', 'look'),\n",
       " ('open', 'rafters'),\n",
       " ('tall', 'tables'),\n",
       " ('traditional', 'booths'),\n",
       " ('glendale', 'avenue'),\n",
       " ('front', 'door'),\n",
       " ('pretty', 'simple'),\n",
       " ('simple', 'menu'),\n",
       " ('upscale', 'options'),\n",
       " ('ice-cream', 'shakes'),\n",
       " ('first', 'thing'),\n",
       " ('nice', 'twist'),\n",
       " ('fast-food', 'burger'),\n",
       " ('grilled', 'onions'),\n",
       " ('sit-down', 'i'),\n",
       " ('iced', 'tea'),\n",
       " ('small', 'condiment'),\n",
       " ('so', 'i'),\n",
       " ('nice', 'gentleman'),\n",
       " ('pretty', 'standard'),\n",
       " ('fast-food', 'quality'),\n",
       " ('good', 'dipped'),\n",
       " ('good', 'meal'),\n",
       " ('best', 'carne'),\n",
       " ('ever', 'had.this'),\n",
       " ('had.this', 'place'),\n",
       " ('really', 'horrible'),\n",
       " ('high', 'school'),\n",
       " ('very', 'small'),\n",
       " ('flavorful', 'thing'),\n",
       " ('low', 'grade'),\n",
       " ('uncomfortable', 'chairs'),\n",
       " ('actually', 'angry'),\n",
       " ('good', 'reviews'),\n",
       " ('carne', 'asada'),\n",
       " ('closest', 'roach'),\n",
       " ('real', 'carne'),\n",
       " ('small', 'hidden'),\n",
       " ('really', 'cozy'),\n",
       " ('table', 'bump'),\n",
       " ('pernil', 'asado..awesome'),\n",
       " ('mashed', 'potato'),\n",
       " ('bit', 'hard'),\n",
       " ('totally', 'worth'),\n",
       " ('i', 'm'),\n",
       " ('friday', 'night'),\n",
       " ('so', 'massive'),\n",
       " ('overpriced-cafeteria', 'flavor'),\n",
       " ('fe', 'green'),\n",
       " ('green', 'chile'),\n",
       " ('following', 'ingredients'),\n",
       " ('wood-fired', 'poblano'),\n",
       " ('chipotle', 'aoli'),\n",
       " ('glorious', 'doesn'),\n",
       " ('wood-fired', 'poblano'),\n",
       " ('chipotle', 'aoli'),\n",
       " ('so', 'i'),\n",
       " ('maybe', 'chipotle'),\n",
       " ('chipotle', 'aoli'),\n",
       " ('smell', 'taste'),\n",
       " ('so', 'close'),\n",
       " ('friday', 'night'),\n",
       " ('very', 'busy'),\n",
       " ('never', 'half'),\n",
       " ('open', 'booths'),\n",
       " ('sophia', 'loren'),\n",
       " ('small', 'crab'),\n",
       " ('sweet', 'crunchy'),\n",
       " ('only', 'problem'),\n",
       " ('barely', 'warm..almost'),\n",
       " ('warm..almost', 'cool'),\n",
       " ('main', 'courses'),\n",
       " ('truffle', 'butter'),\n",
       " ('orgasmic', 'truffle'),\n",
       " ('bit', 'small'),\n",
       " ('so', 'good'),\n",
       " ('big', 'plate'),\n",
       " ('doesn', 't'),\n",
       " ('very', 'moist'),\n",
       " ('just', 'right'),\n",
       " ('so', 'many'),\n",
       " ('many', 'times'),\n",
       " ('times', 'blackened'),\n",
       " ('blackened', 'items'),\n",
       " ('fish-fish', 'eater'),\n",
       " ('truffled', 'mac'),\n",
       " ('entire', 'meal'),\n",
       " ('guilty', 'pleasure'),\n",
       " ('monstrously', 'huge'),\n",
       " ('black', 'raspberry'),\n",
       " ('very', 'nice'),\n",
       " ('nice', 'presentation'),\n",
       " ('quite', 'bitter.in'),\n",
       " ('bitter.in', 'terms'),\n",
       " ('very', 'friendly'),\n",
       " ('bit', 'slow'),\n",
       " ('fairly', 'new'),\n",
       " ('other', 'dishes'),\n",
       " ('great', 'smile'),\n",
       " ('problem.bottom', 'line'),\n",
       " ('great', 'seafood'),\n",
       " ('=', '='),\n",
       " ('weekday', 'lunch'),\n",
       " ('so', 'i'),\n",
       " ('strongest', 'feature'),\n",
       " ('regular', 'basis'),\n",
       " ('happy', 'hour'),\n",
       " ('small', 'plates'),\n",
       " ('free', 'mediocre'),\n",
       " ('actual', 'dinner'),\n",
       " ('other', 'side'),\n",
       " ('happy', 'hour'),\n",
       " ('different', 'servers'),\n",
       " ('other', 'tables'),\n",
       " ('happy', 'hour'),\n",
       " ('dozen', 'other'),\n",
       " ('mexican', 'restaurants'),\n",
       " ('better', 'food'),\n",
       " ('tamale', 's'),\n",
       " ('saturday', 'afternoon'),\n",
       " ('cozy', 'spot'),\n",
       " ('cold', 'beer'),\n",
       " ('top', 'sushi'),\n",
       " ('entire', 'phoenix'),\n",
       " ('friendly', 'staff'),\n",
       " ('staff', 'fresh'),\n",
       " ('fresh', 'sushi'),\n",
       " ('sushi', 'reasonable'),\n",
       " ('reasonable', 'prices'),\n",
       " ('great', 'sushi'),\n",
       " ('good', 'things'),\n",
       " ('not', 'disappointed'),\n",
       " ('ten', 'minutes'),\n",
       " ('missing', 'star'),\n",
       " ('prickly', 'pear'),\n",
       " ('very', 'good'),\n",
       " ('comfortable', 'dinner'),\n",
       " ('next', 'day'),\n",
       " ('jalapeno', 'cornbread'),\n",
       " ('cornbread', 'casserole'),\n",
       " ('less', 'impressive'),\n",
       " ('mashed', 'potatoes'),\n",
       " ('wait', 'staff'),\n",
       " ('very', 'attentive'),\n",
       " ('minor', 'glitch'),\n",
       " ('happy', 'hour'),\n",
       " ('good', 'food'),\n",
       " ('food', 'pleasant'),\n",
       " ('pleasant', 'people'),\n",
       " ('great', 'drinks'),\n",
       " ('almost', 'ruined'),\n",
       " ('so', 'i'),\n",
       " ('glazed', 'pecans'),\n",
       " ('turkey', 'sausage'),\n",
       " ('hard', 'grizzly'),\n",
       " ('decent', 'spot'),\n",
       " ('good', 'greek'),\n",
       " ('here', 'many'),\n",
       " ('many', 'times'),\n",
       " ('bad', 'meal.gyro'),\n",
       " ('very', 'flavorful'),\n",
       " ('great', 'texture'),\n",
       " ('best', 'i'),\n",
       " ('even', 'rare'),\n",
       " ('in-n-out', 'location'),\n",
       " ('minimum', 'wage'),\n",
       " ('minimum', 'effort'),\n",
       " ('t', 'true'),\n",
       " ('random', 'temperature'),\n",
       " ('specific', 'doneness'),\n",
       " ('lazy', 'employees'),\n",
       " ('bad', 'examples'),\n",
       " ('relatively', 'higher-tier'),\n",
       " ('fast-food', 'burgers.if'),\n",
       " ('other', 'restaurant'),\n",
       " ('mexican', 'comfort'),\n",
       " ('very', 'first'),\n",
       " ('first', 'place'),\n",
       " ('worst', 'service'),\n",
       " ('so', 'easy'),\n",
       " ('friday', 'night'),\n",
       " ('few', 'appetizers'),\n",
       " ('tomato', 'bisquechocolate'),\n",
       " ('s', 'mores'),\n",
       " ('graham', 'crackers'),\n",
       " ('toasted', 'marshmallow'),\n",
       " ('very', 'attentive'),\n",
       " ('extremely', 'busy'),\n",
       " ('crowded', 'feeling'),\n",
       " ('more', 'communal'),\n",
       " ('own', 'house'),\n",
       " ('best', 'choice'),\n",
       " ('decent', 'thai'),\n",
       " ('large', 'group'),\n",
       " ('attentive-', 'making'),\n",
       " ('making', 'sure'),\n",
       " ('entire', 'party'),\n",
       " ('medium', 'hot'),\n",
       " ('hot', 'spiciness'),\n",
       " ('only', 'complaint'),\n",
       " ('really', 'tiny'),\n",
       " ('tiny', 'pieces-'),\n",
       " ('big', 'tv'),\n",
       " ('random', 'karaoke'),\n",
       " ('actual', 'videos'),\n",
       " ('big', 'deal'),\n",
       " ('steady', 'stream'),\n",
       " ('take-out', 'orders'),\n",
       " ('normally', 'i'),\n",
       " ('extra', 'star'),\n",
       " ('olive', 'gardens'),\n",
       " ('long', 'list'),\n",
       " ('i', 'work'),\n",
       " ('south', 'west'),\n",
       " ('only', 'other'),\n",
       " ('other', 'option'),\n",
       " ('tex', 'mex'),\n",
       " ('wrong', 'i'),\n",
       " ('pretty', 'good'),\n",
       " ('good', 'ones'),\n",
       " ('blue', 'corn'),\n",
       " ('guacamole', 'salsa'),\n",
       " ('blue', 'corn'),\n",
       " ('turkey', 'meat'),\n",
       " ('moist', 'yummy'),\n",
       " ('right', 'amount'),\n",
       " ('green', 'chile'),\n",
       " ('green', 'chile'),\n",
       " ('good', 'job.the'),\n",
       " ('job.the', 'interior'),\n",
       " ('interior', 'looks'),\n",
       " ('cultural', 'museum'),\n",
       " ('very', 'organic'),\n",
       " ('nice', 'touch'),\n",
       " ('too', 'special'),\n",
       " ('hot', 'spot'),\n",
       " ('good', 'food'),\n",
       " ('great', 'ambiance'),\n",
       " ('best', 'dishes'),\n",
       " ('sunday', 'morning'),\n",
       " ('not', 'sure'),\n",
       " ('beautiful', 'place'),\n",
       " ('so', 'romantic'),\n",
       " ('great', 'place'),\n",
       " ('here', 'last'),\n",
       " ('friday', 'night'),\n",
       " ('great', 'way'),\n",
       " ('oh', 'boy'),\n",
       " ('tequila', 'selection'),\n",
       " ('great', 'time'),\n",
       " ('friday', 'night'),\n",
       " ('mexican', 'restaurants'),\n",
       " ('i', 'm'),\n",
       " ('questionable', 'stars'),\n",
       " ('pretty', 'good'),\n",
       " ('best', 'thing'),\n",
       " ('so', 'tasty'),\n",
       " ('skin', 'roll'),\n",
       " ('much', 'better'),\n",
       " ('first', 'time'),\n",
       " ('salmon', 'skin'),\n",
       " ('so', 'i'),\n",
       " ('fried', 'rolls'),\n",
       " ('too', 'fatty'),\n",
       " ('fresh', 'fish'),\n",
       " ('great', 'yelp'),\n",
       " ('very', 'disappointed'),\n",
       " ('pretty', 'good'),\n",
       " ('fish', 'filet'),\n",
       " ('italian', 'dressing'),\n",
       " ('wish', 'bone'),\n",
       " ('not', 'sure'),\n",
       " ('bf', 's'),\n",
       " ('s', 'dish'),\n",
       " ('i', 'hate'),\n",
       " ('so', 'harsh'),\n",
       " ('ta', 'call'),\n",
       " ('little', 'cottage'),\n",
       " ('little', 'leery'),\n",
       " ('great', 'customer'),\n",
       " ('wonderful', 'food'),\n",
       " ('still', 'mad'),\n",
       " ('so', 'glad'),\n",
       " ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning series of lists of bigrams into one list\n",
    "testBigramsAsList = []\n",
    "def addBigrams(bigramList):\n",
    "    for i in bigramList:\n",
    "        testBigramsAsList.append(i)\n",
    "list_of_test_bigrams.map(addBigrams)\n",
    "testBigramsAsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5581"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of unique bigrams (deduping bigramsAsList)\n",
    "bigramsListTest= list(set((testBigramsAsList)))\n",
    "len(bigramsListTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary with how many docs the bigrams appear in within test set\n",
    "bigramDict_test={}\n",
    "bigramDict_test=bigramDict_test.fromkeys(bigramsListTest,0)\n",
    "\n",
    "\n",
    "for bigram in bigramsListTest:\n",
    "    for cell in list_of_test_bigrams:\n",
    "        if bigram in cell:\n",
    "            bigramDict_test[bigram] = bigramDict_test[bigram]+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing bigrams that appear in less than 10 docs\n",
    "finalTestBigramDict = {k:v for k, v in bigramDict_test.items() if v>10}\n",
    "finalTestBigramsList = finalTestBigramDict.keys()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get length of bigrams list\n",
    "len(finalTestBigramsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Transform Train Data for PMI Analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make unigrams list into a series\n",
    "unigramSeries_train = pd.Series(train_unigramsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training bigrams 76\n"
     ]
    }
   ],
   "source": [
    "# Turn training bigrams into list of strings to use in co-occurance counter\n",
    "bigram_str_list_train = []\n",
    "\n",
    "for i in range(0,len(finalTrainBigramsList)):\n",
    "    s = str(finalTrainBigramsList[i][0]) +\" \"+ str(finalTrainBigramsList[i][1])\n",
    "    bigram_str_list_train.append(s)\n",
    "\n",
    "# Remove duplicate bigrams\n",
    "bigram_str_list_train = list(set(bigram_str_list_train))\n",
    "print \"Number of training bigrams\", len(bigram_str_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update once ready for full lists\n",
    "check_words_train = posMatches_train + negMatches_train\n",
    "reviews_list_train = (train_SSsubset_reviewslist).tolist()\n",
    "\n",
    "#Unigrams and bigrams to find co-occurances with the sentiment words\n",
    "uniBiSeries_train = unigramSeries_train.append(pd.Series(bigram_str_list_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Co-Occurence]\n",
    "The below blocks deals with finding the proximity-based co-occurrences of either bigrams or unigrams that we provide to the function. This is essentially needed for the joint probability portion of the PMI calculation of ngrams and the reference positive or negatives words that we also provide to the function. As you can see in the function definition parameters, we are declaring a proximity distance of 3. This means that we'll be checking for whether or not a reference word is within 3 words or tokens of the chosen ngram (bigram or unigram). The result of this function and the following blocks is to create a dictionary of dictionaries where the keys are the ngrams and the values are nested dictionaries for each ngram with reference word keys and the counts of the times where the reference word was in the correct proximity of that ngram. That result is then used for the PMI calculations later on in the code. \n",
    "\n",
    "Our methodology for this function, **coocur**, uses the following logical steps: \n",
    "We bring in the different parameters: a reference word list, a review list, and a list of ngrams (unigrams or bigrams). \n",
    "\n",
    "We then create a dictionary for the reference words and set them all the 0. This is used later on for the nested dictionary. \n",
    "\n",
    "The first major for-loop basically goes through the given review in the list of reviews, finds where the ngrams are, and puts them into a tuple list for further processing.\n",
    "\n",
    "The second for-loop does the proximity analysis using the tokenized review and the ngrams list. It searches for a range around the ngram in the review, and then assesses whether or not each word in the reference list is within that proximity. If a reference word is in the range, the value for the word in the dictionary is incremented by 1. \n",
    "\n",
    "This process across all the reviews, ngrams, and reference words results in the nested dictionary where each (unigram or bigram) is the key, and the nested dictionary is the result of this **coocur** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkReview (review, w, ref_words, correct_tuples_list, dist):\n",
    "    freq_count = {}\n",
    "    \n",
    "    w = w.lower()\n",
    "    review = review.lower()\n",
    "    review_formatted = smart_str((review))\n",
    "    split_rev = word_tokenize(review_formatted)           # List of all words in a review (to use in indexing)\n",
    "    \n",
    "    w_list = []\n",
    "    for i in range(0,len(w.split())):\n",
    "        w_list.append(smart_str(w.split()[i]))            # If a bigram, splits into a list. Unigrams are just a list of 1\n",
    "    \n",
    "    indices1 = np.where(np.array(split_rev) == w_list[0])[0]\n",
    "    \n",
    "    if len(w_list) > 1:                                   # If it is a bigram\n",
    "        indices2 = np.where(np.array(split_rev) == w_list[1])[0]\n",
    "    else:\n",
    "        indices2 = indices1                               # unigrams will be checked left and right of the same index\n",
    "        \n",
    "    for idx2_spot in indices2:\n",
    "        for idx1_spot in indices1:\n",
    "            if (idx2_spot - idx1_spot) <2 :               # complete the same task for 1 (bigrams) or 0 (unigrams)\n",
    "                tuple_indices = (idx1_spot, idx2_spot)\n",
    "                correct_tuples_list.append(tuple_indices)\n",
    "        \n",
    "    for a, b in correct_tuples_list: \n",
    "        if (a-dist) < 0:\n",
    "            start = 0                          # if distance is too short, start at 0\n",
    "        else:\n",
    "            start = a-dist\n",
    "        if (b+dist) > len(split_rev):\n",
    "            end = len(split_rev)-1             # if distance is too long, end at the last index\n",
    "        else:\n",
    "            end = b + dist\n",
    "        for j in ref_words:\n",
    "            if j in split_rev[start:end+1]:    # check if the sentiment word is in the range\n",
    "                freq_count[j] = 1\n",
    "            else:\n",
    "                freq_count[j] = 0\n",
    "    return freq_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coocur (w, ref_words = check_words_train, reviews= reviews_list_train, dist = 3):\n",
    "    w_coocur = dict.fromkeys(ref_words, 0)\n",
    "    correct_tuples_list = []                  # Empty list to store tuple locations of bigrams/unigrams within review\n",
    "    \n",
    "    freq_list = map(functools32.partial(checkReview, w=w, ref_words = ref_words, \\\n",
    "                                      correct_tuples_list = correct_tuples_list, dist = dist), reviews)\n",
    "        \n",
    "    for item in range(0,len(freq_list)):\n",
    "        for k in freq_list[item].keys():\n",
    "            w_coocur[k] = w_coocur[k] + freq_list[item][k]\n",
    "        \n",
    "    return w_coocur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below maps the above function onto a list of our ngrams. It basically replaces the ngrams in the series with the returned dictionary from the **coocur** function. \n",
    "\n",
    "This is the final piece of this proximity analysis. This final block below creates the nested dictionary needed for the PMI analysis. It basically connects the original list of ngrams we used for the **coocur** function with its dictionary of counts with the reference words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-7f23dd2b2af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a list of dictionaries for each unigram/bigram feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlist_dict_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniBiSeries_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoocur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2104\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m         return self._constructor(new_values,\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:62658)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-f907c9dc8118>\u001b[0m in \u001b[0;36mcoocur\u001b[0;34m(w, ref_words, reviews, dist)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorrect_tuples_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m                  \u001b[0;31m# Empty list to store tuple locations of bigrams/unigrams within review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfreq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctools32\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckReview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref_words\u001b[0m\u001b[0;34m,\u001b[0m                                       \u001b[0mcorrect_tuples_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_tuples_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-788cfe068425>\u001b[0m in \u001b[0;36mcheckReview\u001b[0;34m(review, w, ref_words, correct_tuples_list, dist)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mfreq_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mfreq_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfreq_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a list of dictionaries for each unigram/bigram feature\n",
    "list_dict_train = uniBiSeries_train.map(coocur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn unigram/bigram Series into a list\n",
    "uniBiList_train = uniBiSeries_train.tolist()\n",
    "\n",
    "# Attach the unigram/bigram list as the keys for the list of dictionaries\n",
    "count_both_train = dict(zip(uniBiList_train,list_dict_train))\n",
    "count_both_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get bigram strings by review\n",
    "def join_review_bigrams(review_bigrams):\n",
    "    review_bigram_str_list = []\n",
    "    for i in range(0,len(review_bigrams)):\n",
    "        s = review_bigrams[i][0] +\" \"+ review_bigrams[i][1]\n",
    "        review_bigram_str_list.append(s.encode('utf-8'))\n",
    "    return review_bigram_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of bigram and unigram strings in each review\n",
    "train_SSsubset['ReviewBigrams'] = list_of_train_bigrams   # Add column of bigram tuples in each review\n",
    "train_SSsubset['ReviewBigramStrings'] = train_SSsubset['ReviewBigrams'].map(join_review_bigrams) # Get list of bigram strings in each review\n",
    "train_SSsubset['ReviewUnigramStrings'] = train_SSsubset['reviews'].apply(lambda t: t.split()) # Get list of unigram strings in each review\n",
    "train_SSsubset['ReviewStrings'] = train_SSsubset['ReviewBigramStrings'] + (train_SSsubset['ReviewUnigramStrings']) # Combine list of bigram and unigram strings in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update bigram dict to use strings\n",
    "finalTrainBigramStringDict = defaultdict(lambda : defaultdict(int))\n",
    "finalTrainBigramsStrings = join_review_bigrams(finalTrainBigramsList)\n",
    "for i in range(0, len(finalTrainBigramsStrings)):\n",
    "    finalTrainBigramStringDict[finalTrainBigramsStrings[i].lower()] = finalTrainBigramDict[finalTrainBigramsList[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create full lookup term dictionary (unigrams + bigrams)\n",
    "train_lookuptermDict = defaultdict(lambda : defaultdict(int))\n",
    "train_lookuptermDict = train_counts_dict\n",
    "train_lookuptermDict.update(finalTrainBigramStringDict)\n",
    "train_lookuptermDict\n",
    "train_lookupterms = train_lookuptermDict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Transform Test Data for PMI Analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make unigrams list into a series\n",
    "unigramSeries_test = pd.Series(testunigramsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn training bigrams into list of strings to use in co-occurance counter\n",
    "bigram_str_list_test = []\n",
    "\n",
    "for i in range(0,len(finalTestBigramsList)):\n",
    "    s = str(finalTestBigramsList[i][0]) +\" \"+ str(finalTestBigramsList[i][1])\n",
    "    bigram_str_list_test.append(s)\n",
    "\n",
    "# Remove duplicate bigrams\n",
    "bigram_str_list_test = list(set(bigram_str_list_test))\n",
    "len(bigram_str_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update once ready for full lists\n",
    "check_words_test = posMatches_test + negMatches_test\n",
    "reviews_list_test = (test_SSsubset_reviewslist).tolist()\n",
    "\n",
    "#Unigrams and bigrams to find co-occurances with the sentiment words\n",
    "uniBiSeries_test = unigramSeries_test.append(pd.Series(bigram_str_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coocur_test (w, ref_words = check_words_test, reviews= reviews_list_test, dist = 3):\n",
    "    w_coocur = dict.fromkeys(ref_words, 0)\n",
    "    correct_tuples_list = []                  # Empty list to store tuple locations of bigrams/unigrams within review\n",
    "    \n",
    "    freq_list = map(functools32.partial(checkReview, w=w, ref_words = ref_words, \\\n",
    "                                      correct_tuples_list = correct_tuples_list, dist = dist), reviews)\n",
    "        \n",
    "    for item in range(0,len(freq_list)):\n",
    "        for k in freq_list[item].keys():\n",
    "            w_coocur[k] = w_coocur[k] + freq_list[item][k]\n",
    "        \n",
    "    return w_coocur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of dictionaries for each unigram/bigram feature\n",
    "list_dict_test = uniBiSeries_test.map(coocur_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn unigram/bigram Series into a list\n",
    "uniBiList_test = uniBiSeries_test.tolist()\n",
    "\n",
    "# Attach the unigram/bigram list as the keys for the list of dictionaries\n",
    "count_both_test = dict(zip(uniBiList_test,list_dict_test))\n",
    "count_both_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get list of bigram and unigram strings in each review\n",
    "test_SSsubset['ReviewBigrams'] = list_of_train_bigrams   # Add column of bigram tuples in each review\n",
    "test_SSsubset['ReviewBigramStrings'] = test_SSsubset['ReviewBigrams'].map(join_review_bigrams) # Get list of bigram strings in each review\n",
    "test_SSsubset['ReviewUnigramStrings'] = test_SSsubset['reviews'].apply(lambda t: t.split()) # Get list of unigram strings in each review\n",
    "test_SSsubset['ReviewStrings'] = test_SSsubset['ReviewBigramStrings'] + (test_SSsubset['ReviewUnigramStrings']) # Combine list of bigram and unigram strings in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update bigram dict to use strings\n",
    "finalTestBigramStringDict = defaultdict(lambda : defaultdict(int))\n",
    "finalTestBigramsStrings = join_review_bigrams(finalTestBigramsList)\n",
    "for i in range(0, len(finalTestBigramsStrings)):\n",
    "    finalTtestBigramStringDict[finalTestBigramsStrings[i].lower()] = finalTestBigramDict[finalTestBigramsList[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Train PMI Calculations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before calculating PMI, need inputs\n",
    "\n",
    "# Input 1\n",
    "# Total number of documents, used to calculate probability\n",
    "train_tot_n_docs = train_SSsubset.shape[0]\n",
    "\n",
    "# Input 2\n",
    "# List of lookup terms, including unigrams and bi-grams\n",
    "train_lookupterm_termlist = train_lookupterms\n",
    "\n",
    "# Input 3\n",
    "# Dictionary of lookup term document counts\n",
    "train_lookupterm_dict = train_lookuptermDict\n",
    "\n",
    "# Input 4\n",
    "# List of positive sentiment terms\n",
    "train_pos_sentiment_termlist = posMatches_train\n",
    "\n",
    "# Input 5\n",
    "# List of negative sentiment terms\n",
    "train_neg_sentiment_termlist = negMatches_train\n",
    "\n",
    "# Input 6\n",
    "# Dictionary of sentiment term document counts\n",
    "train_sentimentterm_dict = refWordsDictTrain\n",
    "\n",
    "# Input 7\n",
    "# Dictionary of cooccurance for each lookup term (unigrams and bigrams) to sentiment terms (positive and negative)\n",
    "# Format: {LT1: {ST1: freq, ST2: freq, ST3: freq, ST4: freq}, LT2: {ST1: freq, ST2: freq, ST3: freq, ST4: freq}}\n",
    "train_cooccur_dict = count_both_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function calculates the PMI between two terms\n",
    "# Inputs are term counts, joint counts, and total number of documents\n",
    "def train_pmi_calc(term, sentimentword):\n",
    "    prob_term = (train_lookupterm_dict[term] * 1.0) / train_tot_n_docs\n",
    "    prob_sentiment = (train_sentimentterm_dict[sentimentword] * 1.0) / train_tot_n_docs\n",
    "    prob_both = (train_cooccur_dict[term][sentimentword] * 1.0) / train_tot_n_docs\n",
    "    pmi_pair = log2(prob_both / ((prob_term * prob_sentiment) + 0.001))\n",
    "    return pmi_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function calculates the PMI scores for terms in a given list, \n",
    "#      compared against a list of positive sentiment terms and a list of negative sentiment terms\n",
    "# This function references the pmi_calc function previously defined\n",
    "\n",
    "so_scores = {}\n",
    "def train_pmi(term_list, pos_sentiment_list, neg_sentiment_list):\n",
    "    so_scores = defaultdict(lambda : defaultdict(int))\n",
    "    for t in range(len(term_list)):\n",
    "        pos_so = []\n",
    "        neg_so = []\n",
    "        for p in range(len(pos_sentiment_list)):\n",
    "            pos_so.append(train_pmi_calc(term_list[t], pos_sentiment_list[p]) * 1.0)\n",
    "        for n in range(len(neg_sentiment_list)):\n",
    "            neg_so.append(train_pmi_calc(term_list[t], neg_sentiment_list[n]) * 1.0)\n",
    "        pos_so_avg = mean(pos_so)\n",
    "        neg_so_avg = mean(neg_so)\n",
    "        so_scores[term_list[t]] = (pos_so_avg - neg_so_avg)\n",
    "    return so_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate PMI scores for our dataset\n",
    "train_pmi_score_lookup = train_pmi(train_lookupterm_termlist, \n",
    "                                   train_pos_sentiment_termlist, \n",
    "                                   train_neg_sentiment_termlist)\n",
    "\n",
    "# # Output\n",
    "# train_pmi_score_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a given list of terms (e.g., relevant terms from a given review), calculate overall PMI\n",
    "# Overall PMI is considered the average PMI of each of the terms\n",
    "\n",
    "def train_review_pmi(review_terms):\n",
    "    review_pmi_scores = []\n",
    "    for i in range(len(review_terms)):\n",
    "        if review_terms[i] in train_pmi_score_lookup:\n",
    "            review_pmi_scores.append(train_pmi_score_lookup[review_terms[i]])\n",
    "    avg_pmi_score = mean(review_pmi_scores)\n",
    "    return avg_pmi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map review_pmi function to datasubset\n",
    "\n",
    "train_SSsubset['ReviewPMI'] = train_SSsubset['ReviewStrings'].map(review_pmi)\n",
    "\n",
    "# # View Output\n",
    "# train_SSsubset['ReviewPMI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Test PMI Calculations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before calculating PMI, need inputs\n",
    "\n",
    "# Input 1\n",
    "# Total number of documents, used to calculate probability\n",
    "test_tot_n_docs = test_SSsubset.shape[0]\n",
    "\n",
    "# Input 2\n",
    "# List of lookup terms, including unigrams and bi-grams\n",
    "test_lookupterm_termlist = test_lookupterms\n",
    "\n",
    "# Input 3\n",
    "# Dictionary of lookup term document counts\n",
    "test_lookupterm_dict = test_lookuptermDict\n",
    "\n",
    "# Input 4\n",
    "# List of positive sentiment terms\n",
    "test_pos_sentiment_termlist = posMatches_test\n",
    "\n",
    "# Input 5\n",
    "# List of negative sentiment terms\n",
    "test_neg_sentiment_termlist = negMatches_test\n",
    "\n",
    "# Input 6\n",
    "# Dictionary of sentiment term document counts\n",
    "test_sentimentterm_dict = refWordsDictTest\n",
    "\n",
    "# Input 7\n",
    "# Dictionary of cooccurance for each lookup term (unigrams and bigrams) to sentiment terms (positive and negative)\n",
    "# Format: {LT1: {ST1: freq, ST2: freq, ST3: freq, ST4: freq}, LT2: {ST1: freq, ST2: freq, ST3: freq, ST4: freq}}\n",
    "test_cooccur_dict = count_both_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function calculates the PMI between two terms\n",
    "# Inputs are term counts, joint counts, and total number of documents\n",
    "def test_pmi_calc(term, sentimentword):\n",
    "    prob_term = (test_lookupterm_dict[term] * 1.0) / test_tot_n_docs\n",
    "    prob_sentiment = (test_sentimentterm_dict[sentimentword] * 1.0) / test_tot_n_docs\n",
    "    prob_both = (test_cooccur_dict[term][sentimentword] * 1.0) / test_tot_n_docs\n",
    "    pmi_pair = log2(prob_both / ((prob_term * prob_sentiment) + 0.001))\n",
    "    return pmi_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function calculates the PMI scores for terms in a given list, \n",
    "#      compared against a list of positive sentiment terms and a list of negative sentiment terms\n",
    "# This function references the pmi_calc function previously defined\n",
    "\n",
    "so_scores = {}\n",
    "def test_pmi(term_list, pos_sentiment_list, neg_sentiment_list):\n",
    "    so_scores = defaultdict(lambda : defaultdict(int))\n",
    "    for t in range(len(term_list)):\n",
    "        pos_so = []\n",
    "        neg_so = []\n",
    "        for p in range(len(pos_sentiment_list)):\n",
    "            pos_so.append(test_pmi_calc(term_list[t], pos_sentiment_list[p]) * 1.0)\n",
    "        for n in range(len(neg_sentiment_list)):\n",
    "            neg_so.append(test_pmi_calc(term_list[t], neg_sentiment_list[n]) * 1.0)\n",
    "        pos_so_avg = mean(pos_so)\n",
    "        neg_so_avg = mean(neg_so)\n",
    "        so_scores[term_list[t]] = (pos_so_avg - neg_so_avg)\n",
    "    return so_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate PMI scores for our dataset\n",
    "test_pmi_score_lookup = testn_pmi(test_lookupterm_termlist,\n",
    "                                  test_pos_sentiment_termlist,\n",
    "                                  test_neg_sentiment_termlist)\n",
    "\n",
    "# # Output\n",
    "# test_pmi_score_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a given list of terms (e.g., relevant terms from a given review), calculate overall PMI\n",
    "# Overall PMI is considered the average PMI of each of the terms\n",
    "\n",
    "def test_review_pmi(review_terms):\n",
    "    review_pmi_scores = []\n",
    "    for i in range(len(review_terms)):\n",
    "        if review_terms[i] in test_pmi_score_lookup:\n",
    "            review_pmi_scores.append(test_pmi_score_lookup[review_terms[i]])\n",
    "    avg_pmi_score = mean(review_pmi_scores)\n",
    "    return avg_pmi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map review_pmi function to datasubset\n",
    "\n",
    "test_SSsubset['ReviewPMI'] = test_SSsubset['ReviewStrings'].map(test_review_pmi)\n",
    "\n",
    "# # View Output\n",
    "# test_SSsubset['ReviewPMI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create matrices for test and train sets\n",
    "y_train_PMI,X_train_PMI = dmatrices('target~0+ReviewPMI', train_SSsubset)\n",
    "y_test_PMI,X_test_PMI = dmatrices('target~0+ReviewPMI', test_SSsubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Logistic Regression model and predict on test set\n",
    "LRclassifier_PMI = LogisticRegression().fit(X_train_PMI,y_train_PMI)\n",
    "predictions_PMI = LRclassifier_PMI.predict(X_test_PMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the confusion matrix for Logistic Regression on PMI data\n",
    "confusion_matrix_PMI = metrics.confusion_matrix(y_test_PMI, predictions_PMI)\n",
    "print confusion_matrix_PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the accuracy score for Logistic Regression on PMI data\n",
    "accuracy_PMI = metrics.accuracy_score(y_test_PMI, predictions_PMI)\n",
    "print accuracy_PMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | Task E - Results |\n",
    "This was a multi-part problem. We had to: (a) figure out which unigrams and bigrams we would test against the reference words when calculating the PMI of the review, (b) calculate the marginal probabilities of the n-grams and the reference words, (c) set up a proximity factor within the PMI algorithm, (d) calculate the joint probability of the combinations of n-grams/reference words, (e) using the PMIs within a review to calculate the average sentiment score for each review, and (f) using the sentiment scores to classify the ratings. \n",
    "\n",
    "When choosing the unigrams to test, we decided to use words which appeared in at least 3 % of documents. We then used a count vectorizer to create a dictionary storing the unigrams as keys and the number of documents in which that unigram occurs as values. In creating bigrams, we used three POS tagging patterns (noun+adj, adv+adj, and adj+noun). This resulted in over 27,000 bigrams, most of which, we found, only occured in 1 or 2 documents. Therefore, we limited our bigrams to only those which occurred in more than 10 documents (out of our training set). We created a similar dictionary of counts to track the marginal probability of these bigrams. We repeated a similar process for reference words, using a lexicon of \"positive\" and \"negative\" words from the Internet (only keeping the words which actually appeared in our corpus). \n",
    "\n",
    "After obtaining marginal probabilities of individual n-grams and reference words, we built a function to get joint probability of the combinations of n-grams with reference words (more detail on this above by the function). The resulting nested dictionary is used for the PMI analysis. \n",
    "\n",
    "For calculating the PMI, we calculated averaged the PMI scores for positive reference words and for negative reference words for each n-gram term.  This was done to cancel out any bias from having more positive reference words than negative reference words.  Then, we subtracted the negative PMI average from the positive PMI average to get an overall PMI score on sentiment for the specific word.  Finally, we pulled PMI scores for relevant n-grams on each review and averaged those to get a PMI score for the review as a whole.  These scores were then used to predict rating using logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task F. \n",
    "\n",
    "### What are the top 5 “attributes” of a restaurant that are associated with (i) high and (ii) low ratings? That is, when people rate a restaurant high or low, are they more likely to mention service, ambiance, etc.? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform X's without stop words\n",
    "X_F = vectorizer_stop.fit_transform(X_B.values)\n",
    "count_df = pd.SparseDataFrame([pd.SparseSeries(X_F[i].toarray().ravel()) \n",
    "                               for i in np.arange(X_F.shape[0])],\n",
    "                              columns = vectorizer_stop.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_df['rating']=data_sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataframe between high and low\n",
    "High_df=count_df[count_df['rating']==1]\n",
    "Low_df=count_df[count_df['rating']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get top 5 'Attributes' associated with High Ratings\n",
    "df_HighRating=pd.DataFrame(High_df.apply(sum))\n",
    "Top5High = df_HighRating.sort(ascending=False, columns=0)[:5]\n",
    "print 'Top 5 Attributes Associated with High Ratings:'\n",
    "print Top5High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get top 5 'Attributes' associated with Low Ratings\n",
    "df_LowRating=pd.DataFrame(Low_df.apply(sum))\n",
    "Top5Low = df_LowRating.sort(ascending=False, columns=0)[:5]\n",
    "print 'Top 5 Attributes Associated with Low Ratings:'\n",
    "print Top5Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## | Task F - Results |\n",
    "People that rated the restaurants with a high rating mentioned words like 'rating','food', 'good', 'place', and 'great'.  Reviews that gave a low rating mentioned the same attributes, but we can see that positive adjectives were mentioned much less frequently. 'Food' and 'place' seem to be top concerns for both parties. There is obviously lots of overlap of words used in good and bad restaurant reviews, as there should be, but context is key in determining sentiment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
